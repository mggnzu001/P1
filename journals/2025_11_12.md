# Comprehensive Algorithm Flashcards
- ## 1. Fundamentals
- ### What are algorithms? #card
  A **sequence** of **unambiguous instructions** for solving a **well-defined** problem.
  
  **Example:** A recipe is an algorithm - it provides step-by-step instructions to create a dish.
  
  ---
- ### What are the defining features of Algorithms? #card
  1. **Finite** - Terminates after a finite number of steps
  2. **Definite** - Rigorously and unambiguously specified
  3. **Input** - Valid inputs are clearly specified
  4. **Output** - Can be proved to produce correct output given valid input
  5. **Effective** - Steps are sufficiently simple and basic
  
  **Example:** A valid algorithm to find the maximum in an array must eventually stop (finite), each step must be clear (definite), must specify it works on arrays (input), must return the largest value (output), and use basic operations like comparison (effective).
  
  ---
- ### Additional important notions about Algorithms #card
- Each step must be **unambiguous** (no room for interpretation)
- The **range of inputs** must be specified clearly (e.g., "positive integers only")
- The **same algorithm** can be represented in different ways (pseudocode, flowchart, code)
- **Several algorithms** for solving the same problem may exist with different properties (time/space trade-offs)
  
  ---
- ### When is it pointless finding the fastest algorithm? #card
  1. The system is **not bottlenecked** at the algorithm's target operation
  2. The program is **not run often** (one-time scripts)
  3. **Time is not a major issue** (processing can happen overnight)
  
  **Example:** Optimizing a script that runs once a year to generate a report that takes 5 minutes is usually not worth the engineering time.
  
  ---
- ### Algorithm design basics #card
  **Specification:**
- Often written in **Pseudocode**
- Take note of **data structures** used
  
  **Design Principles:**
- **Simple** - Easy to understand and implement
- **General** - Works for a wide range of inputs
- **Optimal** - No algorithm can perform better (if possible)
  
  **Analysis:**
- Formal algorithm analysis examines **efficiency in time and space**
  
  ---
- ### Fibonacci Piecewise function #card
  $$
  F(n) = 
  \begin{cases}
  0 & \text{if } n = 0 \\
  1 & \text{if } n = 1 \\
  F(n-1) + F(n-2) & \text{if } n > 1
  \end{cases}
  $$
  
  **Example:** F(5) = F(4) + F(3) = 3 + 2 = 5
  Sequence: 0, 1, 1, 2, 3, 5, 8, 13, 21...
  
  ---
- ### Typical Problem Domains #card
  1. **Sorting and searching** - Organizing and finding data
  2. **String processing** - Pattern matching, text manipulation
  3. **Graph problems** - Network routing, social networks
  4. **Combinatorial Problems** - Permutations, combinations
  5. **Geometric Problems** - Closest pair, convex hull
  6. **Numerical Problems** - Matrix operations, polynomial evaluation
  
  ---
- ## 2. Algorithm Design Strategies
- ### Types of Algorithms #card
  1. **Brute Force** - Try all possibilities exhaustively
  2. **Decrease and Conquer** - Solve larger instances in terms of smaller ones
  3. **Divide and Conquer** - Break problems into distinct subproblems
  4. **Transform and Conquer** - Convert one problem into another
  5. **Space/Time Trading** - Use additional data structures for speed
  6. **Dynamic Programming** - Break problems into overlapping subproblems, store solutions
  7. **Greedy** - Repeatedly do what's best for now (locally optimal choices)
  
  ---
- ## 3. Complexity Analysis
- ### Why do we leave out lower order terms when calculating basic computer steps? #card
  The **high order term** will overpower the lower terms significantly as $n$ grows.
  
  **Example:** For $f(n) = 3n^2 + 100n + 500$:
- When n = 10: $3(100) + 1000 + 500 = 1800$ (lower terms matter)
- When n = 1000: $3(1000000) + 100000 + 500 = 3100500$ (dominated by $n^2$ term)
- When n = 10000: $3(100000000) + 1000000 + 500 ≈ 300000000$ ($n^2$ is 99.7% of total)
  
  ---
- ### Define Big O (Big-Oh) functions #card
  $$
  f = O(g) \text{ if there exists } c > 0 \text{ and } n_0 \geq 0
  $$
  $$
  \text{such that } f(n) \leq c \times g(n) \text{ for all } n \geq n_0
  $$
  
  **Meaning:** $f$ grows **at most** as fast as $g$ (upper bound)
  
  **Example:** $3n + 5 = O(n)$ because for $c = 4$ and $n_0 = 5$, we have $3n + 5 \leq 4n$ for all $n \geq 5$
  
  ---
- ### Define Big Ω (Big-Omega) functions #card
  $$
  f = \Omega(g) \text{ if there exists } c > 0 \text{ and } n_0 \geq 0
  $$
  $$
  \text{such that } f(n) \geq c \times g(n) \text{ for all } n \geq n_0
  $$
  
  **Meaning:** $f$ grows **at least** as fast as $g$ (lower bound)
  
  **Example:** $5n^2 + 3n = \Omega(n^2)$ because for $c = 5$ and $n_0 = 1$, we have $5n^2 + 3n \geq 5n^2$
  
  ---
- ### Define Big Θ (Big-Theta) functions #card
  $$
  f = \Theta(g) \text{ if there exist } c_0, c_1 > 0 \text{ and } n_0 \geq 0
  $$
  $$
  \text{such that } c_1 \times g(n) \leq f(n) \leq c_0 \times g(n) \text{ for all } n \geq n_0
  $$
  
  **Meaning:** $f$ grows **exactly** as fast as $g$ (tight bound)
  
  **Example:** $3n^2 + 2n = \Theta(n^2)$ because we can find constants where $c_1 n^2 \leq 3n^2 + 2n \leq c_0 n^2$
  
  ---
- ### Summary of Big O, Ω, Θ notations #card
- **Big O**: Upper bound (≤) - "at most this fast"
- **Big Ω**: Lower bound (≥) - "at least this fast"
- **Big Θ**: Tight bound (=) - "exactly this fast"
  
  **Example with running times:**
- Average case: $O(n^2)$ - grows at most as $n^2$
- Worst case: $\Omega(2^n)$ - grows at least as $2^n$
- Best case: $\Theta(n)$ - grows exactly as $n$
  
  ---
- ### What are the rules to simplify functions by omitting dominated terms? #card
  1. **Multiplicative constants** can be omitted: $14n^2 = n^2$
  2. **$n^a$ dominates $n^b$** if $a > b$: $n^2$ dominates $n$
  3. **Any exponential dominates any polynomial**: $3^n$ dominates $n^5$
  4. **Any polynomial dominates any logarithm**: $n$ dominates $(log n)^3$; $n$ dominates $n \log n$ is FALSE but $n^2$ dominates $n \log n$
  
  **Common ordering (slowest to fastest):**
  $$\Theta(1) < \Theta(\log n) < \Theta(n) < \Theta(n \log n) < \Theta(n^2) < \Theta(n^3) < \Theta(2^n) < \Theta(n!)$$
  
  ---
- ### Steps for calculating efficiency #card
	- 1. **Identify the algorithm's basic operations**
		- Operations repeated at the core: comparisons, swaps, multiplications, additions
	- 2. **Find an equation** that tallies these operations as a function of $n$
	- 3. **Solve the equation** using summation formulas or recurrence relations
	- **Example - Finding max in array:**
	  1. Basic operation: comparison ($A[i] > max$)
	  2. Equation: $C(n) = n - 1$ (compare each element once)
	  3. Solution: $C(n) \in \Theta(n)$
	  
	  ---
- ### Some Simple Math Tools #card
  $$
  \text{Sum of first n natural numbers: } \sum_{i=1}^{n} i = \frac{n(n+1)}{2} \approx \frac{n^2}{2} \in \Theta(n^2)
  $$
  
  $$
  \text{Index shifting: } \sum_{i=j}^{n} 1 = n - j + 1
  $$
  
  $$
  \text{Constant factor: } \sum_{i=1}^{u} c = c \cdot u
  $$
  
  $$
  \text{Linearity: } \sum_{i=1}^{u} (a_i \pm b_i) = \sum_{i=1}^{u} a_i \pm \sum_{i=1}^{u} b_i
  $$
  
  **Example:** Nested loop analysis:
  ```
  for i = 1 to n:
    for j = i to n:
        // constant operation
  ```
  Inner loop runs: $(n) + (n-1) + ... + 1 = \frac{n(n+1)}{2} \in \Theta(n^2)$
  
  ---
- ### Useful theorem for sequential algorithms #card
  If $t_1(n) \in O(g(n))$ and $t_2(n) \in O(h(n))$ then:
  $$t_1(n) + t_2(n) \in O(\max\{g(n), h(n)\})$$
  
  Same holds for $\Theta$ and $\Omega$
  
  **Example:** Bubble sort then search
- Sorting: $O(n^2)$
- Search: $O(n)$
- Combined: $O(\max\{n^2, n\}) = O(n^2)$
  
  The slower operation dominates the total time.
  
  ---
- ## 4. Recurrence Relations
- ### What is a recurrence relation? #card
  A **recursive mathematical function** that defines a sequence based on previous terms.
  
  **Requirements:**
- **Base case** - Starting condition(s)
- **Recursive case** - How to compute from smaller instances
  
  **Example:** Factorial
- Base: $F(0) = 1$
- Recursive: $F(n) = n \times F(n-1)$ for $n > 0$
  
  ---
- ### How do calculations for recursive relations differ from sequential relations? #card
  Recursive relations need to be solved using:
  1. **Backwards substitution** - Manually substitute until pattern emerges
  2. **Master Theorem** - Formula for common divide-and-conquer patterns
  
  Sequential relations use **summation formulas** directly.
  
  ---
- ### Two main recurrence relation forms #card
  1. **$T(n) = aT(n-k) + f(n)$** - Decrease by constant
	- Example: $T(n) = T(n-1) + n$ (sum of integers)
	  
	  2. **$T(n) = aT(n \div b) + f(n)$** - Divide by factor
	- Example: $T(n) = 2T(n/2) + n$ (merge sort)
	  
	  ---
- ### What is backwards substitution? #card
  Using the recurrence relation repeatedly to find a pattern, then solving for the general case.
  
  **Example:** Factorial multiplications
  ```
  M(n) = M(n-1) + 1  (base: M(0) = 0)
  
  M(n) = M(n-1) + 1
     = [M(n-2) + 1] + 1 = M(n-2) + 2
     = [M(n-3) + 1] + 2 = M(n-3) + 3
     ...
     = M(n-k) + k
  
  When k = n: M(n) = M(0) + n = n
  ```
  
  Therefore: $M(n) \in \Theta(n)$
  
  ---
- ### Master Theorem conditions #card
  For $T(n) = aT(n \div b) + f(n)$ where $f(n) \in \Theta(n^d)$ and $T(1) = c$:
  
  **Case 1:** $a < b^d$  
  → $T(n) \in \Theta(n^d)$ (work dominated by current level)
  
  **Case 2:** $a = b^d$  
  → $T(n) \in \Theta(n^d \log n)$ (work evenly distributed)
  
  **Case 3:** $a > b^d$  
  → $T(n) \in \Theta(n^{\log_b a})$ (work dominated by leaves)
  
  **Example:** Merge Sort: $T(n) = 2T(n/2) + n$
- $a = 2, b = 2, d = 1$
- $a = b^d$ (2 = 2^1) → Case 2
- $T(n) \in \Theta(n \log n)$
  
  ---
- ## 5. Brute Force Algorithms
- ### Define Brute Force algorithms #card
  **Straightforward approaches** directly based on the problem statement, trying all possibilities.
  
  **Characteristics:**
- Simple implementation
- Often inefficient
- Easy to understand
- Baseline for comparison
  
  ---
- ### Benefits and drawbacks of brute force algorithms #card
  **Benefits:**
- Quick and easy to implement
- Simple and widely understood
- Can yield reasonable algorithms for small problems
- Good starting point for optimization
  
  **Drawbacks:**
- Often inefficient ($\Theta(n^2)$ or worse)
- Can be infeasibly slow for large inputs
- May not scale well
  
  **Example:** Finding duplicates by comparing every pair: $\Theta(n^2)$ vs sorted approach: $\Theta(n \log n)$
  
  ---
- ### High level Brute Force example for string matching #card
  1. **Align** pattern at beginning of text
  2. **Compare** left to right until:
	- All characters match (success), OR
	- Mismatch found (failure)
	  3. **Shift** pattern one position right and repeat
	  4. Continue until pattern found or end of text reached
	  
	  **Example:**
	  ```
	  Text:    "HELLO WORLD"
	  Pattern: "WOR"
	  
	  Try 1: HEL vs WOR - mismatch at H
	  Try 2: ELL vs WOR - mismatch at E
	  ...
	  Try 7: WOR vs WOR - MATCH!
	  ```
	  
	  ---
- ### Brute Force string matching algorithm #card
  ```
  BruteForceStringMatch(T[0..n-1], P[0..m-1])
  for k ← 0 to n – m do          // for each position in T
    j ← 0
    while j < m and P[j] = T[k+j] do  // compare pattern
      j ← j + 1
    if j = m return k             // found at position k
  return -1                       // not found
  ```
  
  **Efficiency:**
- **Best case:** $\Theta(m)$ - match at start
- **Average case:** $\Theta(n)$ for natural language
- **Worst case:** $\Theta(mn)$ - "aaaa...aab" searching for "aaa...ab"
  
  ---
- ### Brute force closest pair approach #card
  **Problem:** Find the two closest points among $n$ points in 2D space.
  
  **Algorithm:**
  ```
  dmin ← ∞
  for i ← 1 to n-1 do
  for j ← i+1 to n do
    d ← sqrt((xi – xj)² + (yi - yj)²)
    if d < dmin:
      dmin = d
      index1 = i
      index2 = j
  return index1, index2
  ```
  
  **Efficiency:** $\Theta(n^2)$ - compares every pair
  
  **Note:** Can avoid expensive sqrt by comparing squared distances.
  
  ---
- ## 6. Exhaustive Search
- ### What defines exhaustive search? #card
  A **brute force solution** to searching for an element with a particular property, usually among:
- **Permutations** (different orderings)
- **Combinations** (different selections)
  
  **Process:**
- Generate each element in the problem domain systematically
- Check if it satisfies the constraints
- Track the best solution found
  
  ---
- ### Brute force method for exhaustive searching #card
  1. **Construct** a way to list all potential solutions systematically
  2. **Evaluate** solutions one by one, tracking the best
  3. **Return** the winner when search ends
  
  **Example:** Finding best schedule
  1. Generate all possible schedules
  2. Check each for conflicts and cost
  3. Return lowest-cost valid schedule
  
  ---
- ### Define the Travelling Salesman Problem #card
  **Given:**
- $n$ cities with known distances between each pair
  
  **Find:**
- The shortest tour that visits each city exactly once before returning to start
  
  **Alternative description:** Find the minimum-weight Hamiltonian circuit in a weighted graph.
  
  **Example:** 4 cities A,B,C,D
- Possible tours: ABCD, ABDC, ACBD, ACDB, ADBC, ADCB (and reverses)
- Find tour with minimum total distance
  
  ---
- ### Brute force solution to Travelling Salesman Problem #card
  1. **List all solutions**
	- Generate all $(n-1)!/2$ possible tours
	- Use graph traversal to enumerate paths
	  
	  2. **Evaluate each**
	- Calculate total distance for each tour
	- $\sum$ distances of all edges in tour
	  
	  3. **Return winner**
	- Tour with minimum total distance
	  
	  **Efficiency:** $\Theta(n!)$ - infeasible for large $n$
- 10 cities: 181,440 tours
- 20 cities: 60,822,550,204,416,000 tours
  
  ---
- ### Define the Knapsack Problem #card
  **Given:**
- $n$ items with weights $w_1, w_2, ..., w_n$
- Values $v_1, v_2, ..., v_n$
- Knapsack capacity $W$
  
  **Find:**
- Most valuable subset of items that fit in knapsack
  
  **Example:**
- Items: {(w=2,v=3), (w=3,v=4), (w=4,v=5)}
- Capacity: W=5
- Best: Take items 1&2 (weight=5, value=7)
  
  ---
- ### Brute force solution to Knapsack Problem #card
  1. **List all solutions**
	- Generate all $2^n$ combinations of items
	- Can use bit strings: 101 = items 1 and 3
	  
	  2. **Evaluate each**
	- Check total weight ≤ $W$
	- Calculate total value
	  
	  3. **Return answer**
	- Combination with highest value that fits
	  
	  **Efficiency:** $\Theta(2^n)$ - exponential
	  
	  ---
- ### Define the Assignment Problem #card
  **Given:**
- $n$ people need $n$ jobs done
- Each person assigned exactly 1 job
- Cost of person $i$ doing job $j$ is $C[i,j]$
  
  **Find:**
- Assignment with minimum total cost
  
  **Example:** 3 workers, 3 tasks
  ```
  Cost matrix C:
     Job1  Job2  Job3
  P1    9     2     7
  P2    6     4     3
  P3    5     8     1
  ```
  Best: P1→Job2, P2→Job3, P3→Job1 (cost = 2+3+5 = 10)
  
  ---
- ### Brute force solution to Assignment Problem #card
  1. **List all solutions**
	- Generate all $n!$ permutations
	- Each permutation is an assignment
	  
	  2. **Evaluate each**
	- Sum costs: $\sum_{i=1}^{n} C[i, \pi(i)]$
	  
	  3. **Return cheapest**
	  
	  **Efficiency:** $\Theta(n!)$
	  
	  ---
- ### How do graphs relate to exhaustive search? #card
  Exhaustive search **traverses all vertices** of the graph to explore all possibilities.
  
  **Two main traversals:**
  1. **Depth First Search (DFS)** - Go deep, backtrack when stuck
  2. **Breadth First Search (BFS)** - Explore level by level
  
  **Efficiency:**
- **Adjacency matrix:** $\Theta(|V|^2)$
- **Adjacency list:** $\Theta(|V| + |E|)$
  
  ---
- ### Characteristics of Exhaustive Search #card
  **Limitations:**
- Time is usually infeasible ($O(2^n)$ or $O(n!)$)
- Better algorithms usually exist
  
  **When to use:**
- Sometimes it's the only known solution
- Small problem sizes
- Can use parallel computing to speed up
- Need to verify no better solution exists
  
  ---
- ## 7. Decrease and Conquer
- ### Define Decrease and Conquer strategies #card
  **Approach:**
  1. Solve smaller instance(s) of the problem
  2. Extend solution to the larger problem
  
  **Key difference from Divide & Conquer:**
- Decrease & Conquer **throws away** what it doesn't need
- Divide & Conquer solves **all** subproblems
  
  ---
- ### Three variants of Decrease and Conquer #card
  1. **Decrease by a constant** (usually by 1)
	- Example: Insertion sort, factorial
	  
	  2. **Decrease by a constant factor** (usually by 2)
	- Example: Binary search, fake coin problem
	  
	  3. **Variable size decrease**
	- Example: Euclid's GCD, interpolation search
	  
	  ---
- ### Strengths and weaknesses of Decrease and Conquer #card
  **Strengths:**
- Can be implemented **top-down** (recursive) or **bottom-up** (iterative)
- Often very efficient
- Simpler than divide & conquer
  
  **Weaknesses:**
- Less widely applicable than other strategies
- May not work for all problems
  
  ---
- ### List Decrease by Constant algorithms #card
- **Insertion Sort**
- **Graph Searching** (DFS, BFS)
- **Generating Permutations and Subsets**
- **Topological Sort**
  
  ---
- ### List Decrease by Constant Factor algorithms #card
- **Binary Search**
- **Fake Coin Problem**
- **Multiplication à la Russe (Russian Peasant)**
  
  ---
- ### List Variable Size Decrease algorithms #card
- **Euclid's GCD Algorithm**
- **Interpolation Search**
- **Finding the kth Order Statistic**
  
  ---
- ### Describe Decrease by Constant solution for Insertion Sort #card
  **Approach:**
  1. Assume we have a sorted list of size $n-1$
  2. Insert the $n$-th element in correct position
  3. But first, recursively sort the smaller list
  
  **Bottom-up reasoning:** Start with base case (empty/single element), then build up.
  
  **Example:**
  ```
  [5, 2, 4, 6, 1, 3]
  Start: []
  Add 5: [5]
  Add 2: [2, 5]
  Add 4: [2, 4, 5]
  Add 6: [2, 4, 5, 6]
  Add 1: [1, 2, 4, 5, 6]
  Add 3: [1, 2, 3, 4, 5, 6]
  ```
  
  ---
- ### How is bottom-up different from top-down? #card
  **Top-down (Recursive thinking):**
- Start with full problem
- Break down into smaller pieces
- Solve smaller pieces first
- Combine results
  
  **Bottom-up (Iterative thinking):**
- Start with smallest case (base case)
- Build solution incrementally
- Each step adds one more element
- Never break down the problem
  
  **Example:** Insertion Sort
- **Top-down:** "Sort n-1 elements, then insert nth"
- **Bottom-up:** "Start with 1 element (sorted), keep adding elements"
  
  ---
- ### Insertion Sort algorithm #card
  ```
  insertionSort(Array arr, int n)
  if (n <= 1) return             // base case
  
  insertionSort(arr, n-1)        // sort first n-1
  
  // Insert last element
  last = arr[n-1]
  j = n-2
  while (j >= 0 and arr[j] > last)
    arr[j+1] = arr[j]            // shift right
    j = j - 1
  arr[j+1] = last                // place element
  ```
  
  **Efficiency:**
- **Best case:** $\Theta(n)$ - already sorted
- **Average case:** $\Theta(n^2)$
- **Worst case:** $\Theta(n^2)$ - reverse sorted
  
  ---
- ### Define the Topological Sort Problem #card
  **Given:**
- A Directed Acyclic Graph (DAG)
  
  **Find:**
- An ordering of vertices such that for every edge $u \to v$, vertex $u$ appears before $v$ in the ordering
  
  **Note:** Multiple valid orderings may exist
  
  **Example:** Course prerequisites
- CS101 → CS201 → CS301
- CS101 → CS202
- Valid order: CS101, CS201, CS202, CS301 (or CS101, CS202, CS201, CS301)
  
  ---
- ### Decrease by Constant solution for Topological Sort #card
  **Algorithm:**
  1. **Identify** a source vertex (no incoming edges)
  2. **Remove** that vertex and its outgoing edges
  3. **Append** vertex to sorted list
  4. **Repeat** until no vertices remain
  
  **If no source exists:** Graph has a cycle (not a DAG)
  
  **Efficiency:** $\Theta(|V|^2)$ for adjacency matrix, $\Theta(|V| + |E|)$ for adjacency list
  
  ---
- ### Define the Generating Powerset Problem #card
  **Given:**
- A set with $n$ elements
  
  **Find:**
- All $2^n$ subsets (including empty set and full set)
- Order doesn't matter, no duplicates
  
  **Example:** Set {a, b, c}
  ```
  Powerset: {}, {a}, {b}, {c}, {a,b}, {a,c}, {b,c}, {a,b,c}
  ```
  
  ---
- ### Decrease by Constant solution for Generating Powerset #card
  **Recursive approach:**
  1. Start with base case: PowerSet({}) = {{}}
  2. Recursively decrease set size by removing one element
  3. When returning, append removed element to all existing subsets
  
  **Example with {a,b,c}:**
  ```
  PowerSet({}) = {{}}
  PowerSet({a}) = {{}, {a}}
  PowerSet({a,b}) = {{}, {a}, {b}, {a,b}}
  PowerSet({a,b,c}) = {{}, {a}, {b}, {a,b}, {c}, {a,c}, {b,c}, {a,b,c}}
  ```
  
  **Pattern:** Each step doubles the number of subsets
  
  **Efficiency:** $\Theta(2^n)$ - must generate all subsets
  
  ---
- ### Brute force solution for Generating Powerset #card
  **Bit representation approach:**
  
  1. Assign each element a bit position
  2. Count from $0$ to $2^n - 1$ in binary
  3. Each bit pattern represents a subset
  
  **Example with {a,b,c}:**
  ```
  000 → {}
  001 → {c}
  010 → {b}
  011 → {b,c}
  100 → {a}
  101 → {a,c}
  110 → {a,b}
  111 → {a,b,c}
  ```
  
  **Code pattern:**
  ```
  for i = 0 to 2^n - 1:
  subset = {}
  for j = 0 to n-1:
    if (i & (1 << j)) != 0:
      add element[j] to subset
  ```
  
  ---
- ### How does minimal change version of Generating Powerset differ? #card
  Instead of counting 0,1,2,3... in binary (multiple bits change), use **Gray code** where only one bit changes at a time.
  
  **Binary vs Gray code:**
  ```
  Binary:  Gray:
  000      000
  001      001
  010      011  ← Only 1 bit different from previous
  011      010
  100      110
  101      111
  110      101
  111      100
  ```
  
  **Benefit:** Each subset differs from previous by exactly one element (add or remove)
  
  ---
- ### Define the Generate Permutations Problem #card
  **Given:**
- A set of $n$ distinct elements
  
  **Find:**
- All $n!$ possible orderings
- Each element appears exactly once per permutation
- All permutations are unique
  
  **Example:** {1,2,3}
  ```
  123, 132, 213, 231, 312, 321
  ```
  
  ---
- ### Decrease by Constant solution for Generate Permutations #card
  **Recursive approach:**
  1. Generate all permutations of first $n-1$ elements
  2. Insert the $n$-th element into **all possible positions** of each permutation
  
  **Example building from {1,2} to {1,2,3}:**
  ```
  Perms({1}): [1]
  
  Perms({1,2}): 
  Insert 2 into [1]: [2,1], [1,2]
  
  Perms({1,2,3}):
  Insert 3 into [1,2]: [3,1,2], [1,3,2], [1,2,3]
  Insert 3 into [2,1]: [3,2,1], [2,3,1], [2,1,3]
  ```
  
  **Efficiency:** $\Theta(n \cdot n!)$ - generate $n!$ perms, each takes $O(n)$ to create
  
  ---
- ### For generating permutations, how can minimal change requirement be satisfied? #card
  **Alternating insertion direction:**
  
  Insert from **right** then **left** alternately. This ensures next permutation only requires swapping adjacent elements.
  
  **Example:**
  ```
  Start: 1
  Add 2 (insert R): 12, 21
  Add 3 (insert R then L):
  From 12 (R): 312, 132, 123  ← 3 moves right
  From 21 (L): 321, 231, 213  ← 3 moves left (just swap with previous)
  ```
  
  Notice: 123 → 321 is just a swap, not a complete rearrangement!
  
  ---
- ### Define the goals of the Johnson-Trotter method #card
  **Goals:**
  1. Generate permutations without generating smaller lists first
  2. Use **arrows** to track direction for next permutation
  3. Achieve **minimal change** (only swap adjacent elements)
  
  **Key concept:** Each element has a direction (← or →) indicating where it can "move"
  
  ---
- ### What are the rules of the Johnson-Trotter method? #card
  **Mobile element:** An element $k$ is **mobile** if:
- It has an arrow pointing to an adjacent element
- That adjacent element is **smaller** than $k$
  
  **Example:**
  ```
  ← ← ←
  3 2 1
  
  3 is mobile (points to 2, and 2 < 3)
  2 is mobile (points to 1, and 1 < 2)
  1 is NOT mobile (points left but at boundary)
  ```
  
  ---
- ### Define the Johnson-Trotter algorithm #card
  ```
  Initialize: ← ← ← ... ←
           1 2 3 ... n
  
  while permutation has mobile element:
  1. Find largest mobile element k
  2. Swap k with neighbor it points to
  3. Reverse direction of all elements > k
  4. Add new permutation to list
  
  Return list of permutations
  ```
  
  **Example for {1,2,3}:**
  ```
  ← ← ←
  1 2 3  → Find mobile: 3, swap with 2
  
  ← ← ←
  1 3 2  → Find mobile: 3, swap with 1
  
  ← ← ←
  3 1 2  → Find mobile: 2, swap with 1, reverse dirs for 3
  
  ← → ←
  3 2 1  → Find mobile: 3, swap with 2
  
  → → ←
  2 3 1  → Find mobile: 3, swap with 1
  
  → → ←
  2 1 3  → No mobile elements, done
  ```
  
  **Efficiency:** $\Theta(n!)$ - generates all permutations with minimal changes
  
  ---
- ## 8. Decrease by Constant Factor
- ### Define the Fake Coin Problem #card
  **Given:**
- $n$ coins, one is fake (weighs less)
- A balance scale that compares two **sets** of coins
  
  **Find:**
- The fake coin
  
  **Constraint:** Minimize number of weighings
  
  ---
- ### 2-pile solution to Fake Coin Problem #card
  **Algorithm:**
  1. **Divide** coins into two equal piles
	- Set aside one coin if $n$ is odd
	  2. **Weigh** the two piles
	  3. **Discard** the heavier pile
	  4. **Repeat** until only one coin remains
	  
	  **Example with 9 coins:**
	  ```
	  9 coins → Weigh 4 vs 4 (set aside 1)
	      → Keep lighter 4
	  4 coins → Weigh 2 vs 2
	      → Keep lighter 2
	  2 coins → Weigh 1 vs 1
	      → Keep lighter 1 = FAKE
	  ```
	  
	  **Efficiency:** $\lceil \log_2 n \rceil$ weighings
	  
	  ---
- ### 3-pile solution to Fake Coin Problem #card
  **Algorithm:**
  1. **Divide** coins into three piles (as equal as possible)
  2. **Weigh** first two piles
  3. **Discard:**
	- If equal weight → fake is in third pile
	- If unequal → fake is in lighter pile
	  4. **Repeat** until 1-2 coins remain
	  
	  **Example with 9 coins:**
	  ```
	  9 coins → Weigh 3 vs 3 (third pile: 3)
	      → Keep pile with fake (3 coins)
	  3 coins → Weigh 1 vs 1 (third pile: 1)
	      → Keep pile with fake (1 coin)
	  1 coin → FAKE
	  ```
	  
	  **Efficiency:** $\lceil \log_3 n \rceil$ weighings (better than 2-pile!)
	  
	  ---
- ### Define Multiplication à la Russe (Russian Peasant) #card
  **Problem:** Multiply two numbers $n \times m$
  
  **Algorithm (decrease by factor of 2):**
  $
  f(n,m) = \begin{cases}
  \frac{n}{2} \cdot (2m) & \text{if } n \text{ is even}\\
  \frac{n-1}{2} \cdot (2m) + m & \text{if } n \text{ is odd}
  \end{cases}
  $
  
  **Example:** $13 \times 7$
  ```
  n    m     Add?
  13    7     ✓  (odd)
  6   14        (even)
  3   28     ✓  (odd)
  1   56     ✓  (odd)
  
  Result: 7 + 28 + 56 = 91
  ```
  
  **Pattern:** Halve left, double right; add right when left is odd
  
  **Efficiency:** $\Theta(\log n)$
  
  ---
- ## 9. Variable Size Decrease
- ### Define Euclid's GCD Problem #card
  **Given:** Two positive integers $m$ and $n$
  
  **Find:** The largest integer that divides both $m$ and $n$ exactly (Greatest Common Divisor)
  
  **Example:** gcd(48, 18) = 6
  
  ---
- ### Variable Size Decrease solution for Euclid GCD #card
  **Algorithm:**
  $\text{gcd}(m, n) = \begin{cases}
  m & \text{if } n = 0\\
  \text{gcd}(n, m \bmod n) & \text{if } n > 0
  \end{cases}$
  
  **Example:** gcd(48, 18)
  ```
  gcd(48, 18) → gcd(18, 48 mod 18) = gcd(18, 12)
            → gcd(12, 18 mod 12) = gcd(12, 6)
            → gcd(6, 12 mod 6) = gcd(6, 0)
            → 6
  ```
  
  **Why "variable size"?** The reduction amount $(m \bmod n)$ varies, not constant or constant factor.
  
  **Efficiency:** $O(\log n)$ on average
  
  ---
- ### Define the Find kth Order Statistic Problem #card
  **Given:**
- An unsorted list of $n$ elements
- An integer $k$ where $1 \leq k \leq n$
  
  **Find:**
- The $k$-th smallest element
  
  **Special cases:**
- $k = 1$: minimum
- $k = n$: maximum
- $k = n/2$: median
  
  **Example:** List [7, 3, 9, 1, 5], k=3
  Answer: 5 (third smallest: 1, 3, **5**, 7, 9)
  
  ---
- ### Variable Size Decrease solution to Find kth Order Statistic #card
  **Algorithm (QuickSelect):**
  1. **Pick** a pivot element
  2. **Partition** array into 3 sets:
	- Elements < pivot
	- Elements = pivot
	- Elements > pivot
	  3. **Check position** of pivot:
	- If pivot is at position $k$ → Done!
	- If $k <$ pivot position → Recurse on left partition
	- If $k >$ pivot position → Recurse on right partition
	  
	  **Example:** Find 3rd smallest in [7, 3, 9, 1, 5]
	  ```
	  Pick pivot: 5
	  Partition: [3,1] | [5] | [7,9]
	  Pivot at position 3 → Found! Answer = 5
	  ```
	  
	  **Efficiency:**
- **Best/Average:** $\Theta(n)$
- **Worst:** $\Theta(n^2)$ (bad pivot choices)
  
  ---
- ### Define the Interpolation Search algorithm #card
  **Assumption:** Values in sorted array increase **approximately linearly**
  
  **Algorithm:**
  1. **Model** data as a straight line through endpoints
  2. **Estimate** where target value should be (interpolate)
  3. **Check** that position
  4. **Recurse** on appropriate subarray
  
  **Formula for probe position:**
  $x = \text{low} + \left\lfloor \frac{(k - A[\text{low}])}{(A[\text{high}] - A[\text{low}])} \times (\text{high} - \text{low}) \right\rfloor$
  
  Where $k$ is the search key.
  
  **Example:** Search for 45 in [10, 20, 30, 40, 50, 60]
  ```
  low=0, high=5, k=45
  x = 0 + ((45-10)/(60-10)) × 5 = 3.5 ≈ 3
  Check A[3]=40 < 45
  Continue with low=4, high=5
  ```
  
  **Efficiency:**
- **Best case:** $O(\log \log n)$ with uniform distribution
- **Worst case:** $O(n)$ with skewed distribution
  
  ---
- ## 10. Divide and Conquer
- ### Define the MergeSort algorithm #card
  **Strategy:** Divide array in half, sort each half, then merge sorted halves.
  
  **Algorithm:**
  1. **Divide** array into two halves
  2. **Recursively sort** left half
  3. **Recursively sort** right half
  4. **Merge** the two sorted halves
  
  **Example:** [38, 27, 43, 3, 9, 82, 10]
  ```
  [38,27,43,3,9,82,10]
   ↓ Split
  [38,27,43,3] [9,82,10]
   ↓ Split          ↓ Split
  [38,27] [43,3]   [9,82] [10]
  ↓ Split  ↓ Split    ↓ Split
  [38][27][43][3]   [9][82][10]
  ↓ Merge  ↓ Merge    ↓ Merge
  [27,38] [3,43]    [9,82][10]
   ↓ Merge           ↓ Merge
  [3,27,38,43]      [9,10,82]
         ↓ Merge
   [3,9,10,27,38,43,82]
  ```
  
  **Merging:** Compare front elements of each subarray, take smaller.
  
  **Efficiency:** $\Theta(n \log n)$ for all cases (best, average, worst)
  
  **Trade-off:** Uses $\Theta(n)$ extra space (not in-place)
  
  ---
- ### Define the QuickSort algorithm #card
  **Strategy:** Pick pivot, partition around it, recursively sort partitions.
  
  **Algorithm:**
  1. **Choose** a pivot (often first element)
  2. **Partition** array so:
	- Elements ≤ pivot are on left
	- Elements > pivot are on right
	- Pivot is in final position
	  3. **Recursively** QuickSort left partition
	  4. **Recursively** QuickSort right partition
	  
	  **Example:** [7, 2, 1, 6, 8, 5, 3, 4]
	  ```
	  Pivot: 4
	  Partition: [2,1,3] 4 [6,8,5,7]
	        ↓          ↓
	  Sort left    Sort right
	  ```
	  
	  **Partitioning:** Scan from both ends, swap misplaced elements.
	  
	  ---
- ### Worst-Case Efficiency of QuickSort #card
  **Worst case occurs when:** Splits are completely unbalanced
  
  **Example:** Already sorted array [1,2,3,4,5] with first element as pivot
  ```
  Partition: [] 1 [2,3,4,5]
           → [] 2 [3,4,5]
             → [] 3 [4,5]
               → [] 4 [5]
                 → [] 5 []
  ```
  
  Each partition reduces size by only 1!
  
  **Comparisons:**
  $C_{\text{worst}} = (n+1) + n + ... + 3 = \frac{(n+1)(n+2)}{2} - 3 \in \Theta(n^2)$
  
  **Solution:** Use better pivot selection (median-of-three)
  
  ---
- ### General Efficiency of QuickSort #card
  **Best case:** $\Theta(n \log n)$ - pivot always splits in middle
  
  **Average case:** $\Theta(n \log n)$ - random splits
  
  **Worst case:** $\Theta(n^2)$ - completely skewed splits
  
  **Improvements (20-25% faster combined):**
  1. **Better pivot:** Median-of-three avoids worst case on sorted data
  2. **Switch to Insertion Sort** for small subarrays ($n < 10$)
  3. **Eliminate recursion** with explicit stack
  
  **Why still popular?**
- Good cache performance
- In-place (no extra space like MergeSort)
- Fast average case
- Method of choice for large lists ($n \geq 10000$)
  
  ---
- ### Define the Strassen Matrix Multiplication method #card
  **Standard matrix multiplication:** $\Theta(n^3)$ (three nested loops)
  
  **Strassen's idea:** Reduce 8 recursive multiplications to 7 using clever additions/subtractions
  
  **For 2×2 matrices:**
  $C = A \times B$
  
  **Compute 7 products:**
  ```
  P1 = A11 × (B12 - B22)
  P2 = (A11 + A12) × B22
  P3 = (A21 + A22) × B11
  P4 = A22 × (B21 - B11)
  P5 = (A11 + A22) × (B11 + B22)
  P6 = (A12 - A22) × (B21 + B22)
  P7 = (A11 - A21) × (B11 + B12)
  ```
  
  **Combine results:**
  ```
  C11 = P5 + P4 - P2 + P6
  C12 = P1 + P2
  C21 = P3 + P4
  C22 = P1 + P5 - P3 - P7
  ```
  
  **Efficiency:** $\Theta(n^{2.807})$ - better than standard!
  
  ---
- ### Divide and Conquer solution for Closest Pair #card
  **Algorithm:**
  1. **Sort** points by x-coordinate
  2. **Divide** by vertical line at x-median
  3. **Recursively find** closest pair in left half → $d_L$
  4. **Recursively find** closest pair in right half → $d_R$
  5. Let $d_{\min} = \min(d_L, d_R)$
  6. **Check straddling pairs** within strip of width $2d_{\min}$
	- Only need to check next 5 points in y-sorted order!
	  7. **Update** $d_{\min}$ if closer pair found
	  
	  **Key insight (Packing Argument):**
- In a $d_{\min} \times 2d_{\min}$ rectangle
- Points on each side must be ≥ $d_{\min}$ apart
- Maximum 6 points can exist such that any could be closer than $d_{\min}$
- Therefore, only check next 5 points
  
  **Efficiency:** $\Theta(n \log n)$ 
  
  **Recurrence:** $T(n) = 2T(n/2) + \Theta(n)$ → Master Theorem Case 2
  
  ---
- ### Define Divide and Conquer solution for QuickHull #card
  **Problem:** Find convex hull of $n$ points
  
  **Algorithm:**
  1. **Find** leftmost point $P_1$ and rightmost point $P_2$
  2. **Divide** points by line $P_1P_2$ (upper and lower)
  3. **Compute upper hull:**
	- Find $P_{\max}$ farthest from line $P_1P_2$
	- Recursively QuickHull points left of $P_1P_{\max}$
	- Recursively QuickHull points left of $P_{\max}P_2$
	  4. **Similarly compute lower hull**
	  
	  **"Left of line" test:** Use triangle area via determinant
	  $\text{Area} = 0.5 \times |\det(P_1, P_2, P_{\max})|$
	  
	  **Efficiency:**
- **Average:** $\Theta(n \log n)$
- **Worst:** $\Theta(n^2)$ - when points form nested structure
  
  ---
- ## 11. Transform and Conquer
- ### What are the 3 types of transformations in Transform and Conquer? #card
  
  **1. Instance Simplification** - Transform input to easier form
	- Examples: Presorting, Gaussian elimination
	  
	  **2. Representation Change** - Change data structure
	- Examples: Balanced search trees, heaps, heapsort
	  
	  **3. Problem Reduction** - Convert to different problem
	- Examples: LCM via GCD, reductions to graph problems
	  
	  ---
- ### What are examples of Instance Simplification? #card
  1. **Presorting**
	- Sort data first, then solve problem on sorted data
	- Example: Find duplicates in $O(n)$ instead of $O(n^2)$
	  
	  2. **Gaussian Elimination**
	- Transform matrix to upper triangular form
	- Makes solving systems of equations easier
	  
	  ---
- ### What are examples of Representation Change? #card
  1. **Balanced Search Trees** (AVL, Red-Black)
	- Guarantee $O(\log n)$ operations
	  
	  2. **Heaps and Heapsort**
	- Array representation of binary tree
	- Efficient priority queue
	  
	  3. **Horner's Rule**
	- Polynomial evaluation with fewer multiplications
	  
	  4. **Binary Exponentiation**
	- Compute $a^n$ in $O(\log n)$ instead of $O(n)$
	  
	  ---
- ### What are examples of Problem Reduction? #card
  1. **Lowest Common Multiple via GCD**
	- $\text{LCM}(m,n) = \frac{m \times n}{\text{GCD}(m,n)}$
	  
	  2. **Reductions to Graph Problems**
	- Model problems as graphs
	- Example: River crossing puzzle as state-space graph
	  
	  ---
- ### Define the Gaussian Elimination algorithm #card
  **Goal:** Transform system of linear equations to upper triangular form
  
  **Algorithm:**
  ```
  BetterForwardElimination(A[1..n, 1..n], b[1..n])
  // Append b as last column of A
  for i ← 1 to n do 
    A[i, n+1] ← b[i]
  
  // For each column
  for i ← 1 to n-1 do
    // Find best pivot (largest absolute value)
    pivotrow ← i
    for j ← i+1 to n do
      if |A[j,i]| > |A[pivotrow,i]|:
        pivotrow ← j
    
    // Swap rows
    for k ← i to n+1 do
      swap(A[i,k], A[pivotrow,k])
    
    // Eliminate column below pivot
    for j ← i+1 to n do
      temp ← A[j,i] / A[i,i]
      for k ← i to n+1 do
        A[j,k] ← A[j,k] - A[i,k] × temp
  ```
  
  **Partial Pivoting:** Choosing largest element as pivot reduces numerical errors
  
  **Efficiency:** $\Theta(n^3)$
  
  ---
- ### Where is Presorting valuable? #card
  **Good candidates (faster after presorting):**
- **Searching** - Binary search: $O(\log n)$ vs linear: $O(n)$
- **Computing median** - Direct access after sort
- **Finding repeated elements** - Adjacent duplicates in sorted array
- **Closest pair** - $O(n \log n)$ vs brute force: $O(n^2)$
- **Checking uniqueness** - $O(n)$ scan vs $O(n^2)$ comparisons
  
  **Bad candidates (presorting overhead too expensive):**
- **Finding min/max** - Brute force already $O(n)$
- **Single search** in unordered array - $O(n)$ fine for one search
  
  ---
- ### What is the efficiency of Presorting? #card
  **Overhead:** $\Theta(n \log n)$ for efficient sorting algorithms
  
  **When worth it:**
- Original algorithm is $\Theta(n^2)$ or worse
- Multiple operations on same data
- $\Theta(n \log n)$ + $O(n)$ < $O(n^2)$
  
  **When NOT worth it:**
- Original algorithm already $O(n)$ or better
- One-time operation
  
  ---
- ### When is presorting worth it? #card
  | Problem | Brute Force | With Presorting | Worth it? |
  |---------|-------------|-----------------|-----------|
  | Closest pair | $\Theta(n^2)$ | $\Theta(n \log n)$ | ✓ Yes |
  | Checking uniqueness | $\Theta(n^2)$ | $\Theta(n \log n)$ | ✓ Yes |
  | Finding min/max | $\Theta(n)$ | $\Theta(n \log n)$ | ✗ No |
  | Finding median | $\Theta(n^2)$ | $\Theta(n \log n)$ | ✓ Yes |
  | Single search | $\Theta(n)$ | $\Theta(n \log n)$ | ✗ No |
  | Multiple searches | $\Theta(kn)$ | $\Theta(n \log n + k \log n)$ | ✓ Yes if $k$ large |
  
  ---
- ### Brute force solution for polynomial evaluation #card
  **Problem:** Evaluate $p(x) = a_n x^n + a_{n-1}x^{n-1} + ... + a_1 x + a_0$ at $x = c$
  
  **Brute force:** Directly compute each term
  ```
  p(c) = a_n × c^n + a_{n-1} × c^{n-1} + ... + a_1 × c + a_0
  ```
  
  **Example:** $p(x) = 2x^4 - x^3 + 3x^2 + x - 5$ at $x = 3$
  ```
  = 2×(3^4) - 3^3 + 3×(3^2) + 3 - 5
  = 2×81 - 27 + 3×9 + 3 - 5
  = 162 - 27 + 27 + 3 - 5
  = 160
  ```
  
  **Multiplications needed:**
- $x^4$: 3 multiplications
- $x^3$: 2 multiplications
- $x^2$: 1 multiplication
- Total: $1+2+...+n = \frac{n(n+1)}{2} \in \Theta(n^2)$
  
  ---
- ### How does Horner's Rule work? #card
  **Idea:** Factor out $x$ as much as possible to reduce multiplications
  
  **Formula:** 
  $p(x) = a_0 + x(a_1 + x(a_2 + x(... + x(a_{n-1} + x \cdot a_n)...)))$
  
  **Example:** $p(x) = 2x^4 - x^3 + 3x^2 + x - 5$
  ```
  Step 1: = ((((2)x - 1)x + 3)x + 1)x - 5
  Step 2: Evaluate inside-out at x = 3
  = 2
  = 2×3 - 1 = 5
  = 5×3 + 3 = 18
  = 18×3 + 1 = 55
  = 55×3 - 5 = 160
  ```
  
  **Multiplications:** Exactly $n$ (one per coefficient except first)
  
  **Efficiency:** $\Theta(n)$ vs brute force $\Theta(n^2)$
  
  ---
- ### Horner's Rule pseudocode #card
  ```
  double horner(coefficients[0..n], x):
  p = coefficients[n]              // Start with highest degree
  for i = n-1 downto 0:
    p = x × p + coefficients[i]    // One multiply, one add
  return p
  ```
  
  **Example:** $2x^3 - x^2 + 3x + 1$ at $x = 2$
  ```
  coeffs = [1, 3, -1, 2]  // constant to highest degree
  p = 2                    // Start with a_3
  p = 2×2 + (-1) = 3      // Process a_2
  p = 2×3 + 3 = 9         // Process a_1
  p = 2×9 + 1 = 19        // Process a_0
  ```
  
  **Polynomial where Horner's Rule doesn't help:** $a_0$ (constant) - 0 multiplications either way!
  
  ---
- ### Define the Binary Exponentiation algorithm #card
  **Problem:** Compute $a^n$ efficiently
  
  **Idea:** Use binary representation of $n$
  
  **Example:** $a^{13}$ where $13 = 1101_2 = 2^3 + 2^2 + 2^0$
  $a^{13} = a^8 \times a^4 \times a^1$
  
  **Algorithm:**
  ```
  result = 1
  base = a
  exponent = n
  while exponent > 0:
  if exponent & 1 == 1:      // If bit is 1
    result = result × base
  base = base × base          // Square the base
  exponent = exponent >> 1    // Shift right
  return result
  ```
  
  **Trace for $a^{13}$:**
  ```
  exp (binary)  base    result
  1101         a       1
  110          a²      a        (bit was 1)
  11           a⁴      a        (bit was 0)
  1            a⁸      a⁵       (bit was 1)
  0            a¹⁶     a¹³      (bit was 1)
  ```
  
  **Efficiency:** $\Theta(\log n)$ vs brute force $\Theta(n)$
  
  ---
- ### How is the LCM problem reduced? #card
  **Problem:** Find Least Common Multiple of $m$ and $n$
  
  **Direct approach:** List multiples until match - inefficient!
  
  **Reduction to GCD:**
  $\text{LCM}(m,n) = \frac{m \times n}{\text{GCD}(m,n)}$
  
  **Example:** LCM(24, 60)
  ```
  GCD(24, 60) = 12
  LCM(24, 60) = (24 × 60) / 12 = 1440 / 12 = 120
  ```
  
  **Why it works:**
- LCM contains all prime factors at highest powers
- GCD contains common prime factors
- Product contains factors twice → divide by GCD
  
  **Efficiency:** $O(\log n)$ (cost of GCD) vs potentially $O(n)$ for direct
  
  ---
- ### Explain Sort by Counting #card
  **Problem:** Sort list of integers in range $[L...U]$ where range is small
  
  **Algorithm:**
  1. **Count frequencies** - How many times each value appears
  2. **Calculate distribution** - Cumulative frequencies (positions)
  3. **Place elements** - Use distribution table to place in output
  
  **Example:** Sort [3, 1, 3, 2, 1] (range 1-3)
  ```
  Frequency table:    [2, 1, 2]  (1:2×, 2:1×, 3:2×)
  Distribution table: [2, 3, 5]  (1s end at pos 2, 2s at 3, 3s at 5)
  
  Place elements:
  Input [3,1,3,2,1] → Output [1,1,2,3,3]
  ```
  
  **Efficiency:** $\Theta(n + (U-L))$ - linear when range is small!
  
  **Trade-off:** Requires extra space for counting tables
  
  ---
- ### Sort by Counting algorithm #card
  ```
  CountingSort(A[0..n-1], L, U)
  // Initialize frequency table
  for j ← 0 to U-L do 
    D[j] ← 0
  
  // Count frequencies
  for i ← 0 to n-1 do 
    D[A[i]-L] ← D[A[i]-L] + 1
  
  // Calculate distribution (cumulative)
  for j ← 1 to U-L do 
    D[j] ← D[j-1] + D[j]
  
  // Place elements in output (go backwards for stability)
  for i ← n-1 downto 0 do
    j ← A[i] - L
    S[D[j]-1] ← A[i]
    D[j] ← D[j] - 1
  
  return S
  ```
  
  **Why backwards loop?** Maintains stability (equal elements keep relative order)
  
  ---
- ## 12. String Matching - Advanced
- ### Define Horspool's Algorithm #card
  **Improvement over brute force string matching**
  
  **Key idea:** Use **bad character shift table** based on rightmost character alignment
  
  **Algorithm:**
  1. **Construct shift table** $T$ based on pattern
  2. **Align** pattern with text start
  3. **Compare** from right to left in pattern
  4. On mismatch: **Shift** pattern by $T[c]$ where $c$ is text character aligned with pattern's last character
  
  **Shift table rules:**
- For each character in pattern: distance from rightmost occurrence to end
- For characters not in pattern: pattern length
  
  **Example pattern:** "BARBER"
  ```
  Char:  B  A  R  E  other
  Shift: 2  4  3  1  6
  ```
  
  ---
- ### How does Boyer-Moore differ from Horspool? #card
  **Horspool:** Uses only **bad character shift**
  
  **Boyer-Moore:** Uses both:
  1. **Bad character shift** (like Horspool)
  2. **Good suffix shift** - based on matched portion
  
  **Boyer-Moore takes maximum of both shifts**
  
  **Advantage:** Often shifts farther, potentially faster
  
  **Trade-off:** More complex preprocessing, two tables to maintain
  
  ---
- ### What preprocessing is needed by Boyer-Moore algorithm? #card
  **Two shift tables created from pattern only (before seeing text):**
  
  **1. Bad Symbol Table $T(c)$:**
- Based on each character's position in pattern
- Similar to Horspool
  
  **2. Good Suffix Table $G(k)$:**
- Based on number of matched characters $k$
- Considers what was already matched
  
  **Key insight:** Both tables determined solely by pattern properties, not text
  
  **Example preprocessing for pattern "BAOBAB":**
  ```
  Bad Symbol: Standard rightmost occurrence
  Good Suffix: Depends on matched suffix reoccurrence
  ```
  
  ---
- ### What is the Good Suffix Rule? #card
  **Second shift rule in Boyer-Moore**
  
  Uses $\text{suff}(k)$ = $k$ characters at end of pattern that matched
  
  **Build shift table** $G(k)$ based on matched suffix
  
  **Three cases when building** $G(k)$:
  1. Matching suffix doesn't occur elsewhere → shift entire pattern
  2. Matching suffix occurs earlier → align with that occurrence
  3. Part of suffix matches beginning of pattern → align that part
  
  **Choose:** $\max(\text{bad symbol shift}, \text{good suffix shift})$
  
  ---
- ### Explain Case 1 of Boyer-Moore Good Suffix #card
  **Matched suffix doesn't occur elsewhere in pattern**
  
  **Example:**
  ```
  Text:    ...A B A B...
  Pattern:   M A O B A B
           ↑ mismatch
  ```
  
  $k = 3$ (matched "BAB")
  
  "BAB" doesn't appear earlier in pattern
  
  **Shift:** $G(3) = m = 6$ (entire pattern length)
  
  ```
  After shift:
  Text:    ...A B A B...
  Pattern:         M A O B A B
  ```
  
  ---
- ### Explain Case 2 of Boyer-Moore Good Suffix #card
  **Matched suffix occurs earlier in pattern**
  
  **Case 2A - Same mismatch character:**
  ```
  Text:    ....A B A B...
  Pattern:   O B A B O B A B
             ↑ mismatch at O
  ```
  
  Same 'O' appears earlier → shift past last match
  $G(3) = 8$
  
  **Case 2B - Different mismatch character:**
  ```
  Text:    ....A B A B...
  Pattern:   A B A B O B A B
             ↑ mismatch at A (different from case 2A)
  ```
  
  Different character → can align matched suffix
  $G(3) = 4$
  
  ---
- ### Explain Case 3 of Boyer-Moore Good Suffix #card
  **Part of matched suffix occurs at beginning of pattern**
  
  **Must match from end of suffix!**
  
  **Example:**
  ```
  Text:    .....A B A B...
  Pattern:   A B C B A B
             ↑ mismatch
  ```
  
  $k = 3$ matched ("BAB")
  
  Front "AB" matches end "AB" of suffix
  
  **Shift:** $G(3) = 4$ to align prefix with suffix portion
  
  ```
  After shift:
  Text:    .....A B A B...
  Pattern:       A B C B A B
  ```
  
  ---
- ### Define the Boyer-Moore algorithm #card
  **Algorithm:**
  
  **Preprocessing:**
  1. Build Bad Symbol table $T$
  2. Build Good Suffix table $G$
  
  **Searching:**
  ```
  while pattern not past end of text:
  Compare right to left
  If mismatch after k matches:
    d1 = max(T(c) - k, 1)  // Bad symbol shift
    d2 = G(k) if k > 0     // Good suffix shift
    shift = max(d1, d2)
  If complete match:
    report position
  ```
  
  **Note on $k$ requirement:** Good suffix only used when $k > 0$ (at least one match)
  
  **Best case:** $O(n/m)$ - can skip large portions of text
  
  **Worst case:** $O(nm)$ - but rare in practice
  
  ---
- ## 13. Dynamic Programming
- ### What is the principle of Dynamic Programming? #card
  **Core idea:** Solve problems by storing solutions to **overlapping subproblems**
  
  **Key characteristics:**
  1. Each subproblem solved **only once** and stored for lookup
  2. Falls into **space-time tradeoff** category
  3. Solution derived from **recurrence relation**
  4. Must obey **Principle of Optimality:**
	- Optimal solution can be constructed from optimal solutions to subproblems
	  
	  **Difference from Divide & Conquer:**
- **D&C:** Subproblems are independent
- **DP:** Subproblems overlap (solved multiple times without memoization)
  
  ---
- ### List of Dynamic Programming algorithms #card
  
  **Introductory:**
- Fibonacci
- Change-making problem
- Coin collecting problem
  
  **Graph algorithms:**
- Warshall's Algorithm (Transitive Closure)
- Floyd's Algorithm (All-Pairs Shortest Path)
  
  **Optimization:**
- Knapsack Problem (0/1)
  
  ---
- ### Dynamic Programming algorithm for Fibonacci #card
  ```
  fibo(n):
  F[0] = 0
  F[1] = 1
  for i = 2 to n do:
    F[i] = F[i-1] + F[i-2]
  return F[n]
  ```
  
  **Comparison with naive recursion:**
- **Naive:** $\Theta(1.618^n)$ - exponential!
- **DP:** $\Theta(n)$ - linear!
  
  **Why DP wins:** Naive recalculates F(k) many times; DP calculates once
  
  **Example:** Computing F(5)
  ```
  Naive: F(5) calls F(4) and F(3)
       F(4) calls F(3) and F(2)
       F(3) called TWICE already!
       
  DP: F(0)=0, F(1)=1, F(2)=1, F(3)=2, F(4)=3, F(5)=5
    Each calculated exactly once
  ```
  
  ---
- ### Explain the Change Making Problem #card
  **Given:**
- Coin denominations $d_1 < d_2 < ... < d_m$ (unlimited supply)
- Amount $n$ to make change for
- Always include $d_1 = 1$ (to guarantee solution exists)
  
  **Find:**
- Minimum number of coins that sum to $n$
  
  **Example:**
- Denominations: {1, 5, 10, 25}
- Amount: 30
- Answer: 2 coins (25 + 5)
  
  **Note:** Greedy doesn't always work!
- Denominations: {1, 3, 4}
- Amount: 6
- Greedy: 4 + 1 + 1 = 3 coins
- Optimal: 3 + 3 = 2 coins
  
  ---
- ### Recurrence relation for Change Making Problem #card
  **Let** $F(n)$ = minimum number of coins to make amount $n$
  
  **Base case:**
  $F(0) = 0$
  
  **Recursive case:**
  $F(n) = \min_{j: n \geq d_j} \{F(n - d_j)\} + 1 \text{ for } n > 0$
  
  **Meaning:** Try subtracting each valid denomination, take minimum, add 1 for that coin
  
  **Example:** $n = 6$, denominations {1, 3, 4}
  ```
  F(6) = min{F(6-1), F(6-3), F(6-4)} + 1
     = min{F(5), F(3), F(2)} + 1
     = min{2, 1, 2} + 1
     = 2
  ```
  
  ---
- ### Change Making algorithm #card
  ```
  ChangeMaking(n, D[1..m])
  F[0] ← 0
  for i ← 1 to n do
    temp ← ∞
    j ← 1
    while j ≤ m and i ≥ D[j] do
      temp ← min(F[i - D[j]], temp)
      j ← j + 1
    F[i] ← temp + 1
  return F[n]
  ```
  
  **To reconstruct solution:** Keep track of which coin was used at each step
  
  **Efficiency:** $\Theta(nm)$ where:
- $n$ = amount
- $m$ = number of denominations
  
  ---
- ### Describe the Coin Collecting Problem informally #card
  **Setup:**
- $n \times m$ board (grid)
- Some cells have 1 coin, others have 0
- Robot starts at **top-left**
- Must reach **bottom-right**
- Can only move **right** or **down** (one cell at a time)
- Collects any coin in visited cell
  
  **Goal:** Find maximum coins collectible and the path
  
  **Example:** 3×4 board
  ```
  Start → 0 1 1 0
        1 0 1 0
        0 1 0 1 ← End
  ```
  
  ---
- ### Describe the Coin Collecting Problem formally #card
  **Let** $F(i,j)$ = maximum coins collectible reaching cell $(i,j)$
  
  **Key insight:** Can only reach $(i,j)$ from:
- $(i-1, j)$ - cell above
- $(i, j-1)$ - cell to left
  
  **Recurrence:**
  $F(i,j) = \max(F(i-1,j), F(i,j-1)) + C(i,j)$
  
  Where $C(i,j) \in \{0,1\}$ is coin at cell $(i,j)$
  
  **Base cases:**
- $F(i,0) = 0$ for all $i$ (column 0 not on board)
- $F(0,j) = 0$ for all $j$ (row 0 not on board)
- $F(1,1) = C(1,1)$ (starting cell)
  
  ---
- ### Coin Collecting algorithm solution explained #card
  **Algorithm:**
  1. **Fill table** row by row using recurrence
  2. **Efficiency:** $\Theta(nm)$ time and space
  
  **Backtrace for path:**
- Start at $F(n,m)$ (bottom-right)
- If $F(i-1,j) > F(i,j-1)$ → came from above (↓)
- If $F(i,j-1) > F(i-1,j)$ → came from left (→)
- If equal → either path works
  
  **Backtrace efficiency:** $\Theta(n+m)$
  
  **Example table:**
  ```
     j=1  j=2  j=3  j=4
  i=1   0    1    2    2
  i=2   1    1    3    3
  i=3   1    2    3    4
  
  Backtrace from (3,4):
  (3,4)→(3,3)→(2,3)→(2,2)→(2,1)→(1,1)
  ```
  
  ---
- ## 14. Graph Algorithms - Dynamic Programming
- ### Define a Transitive Closure #card
  **For directed graph with $n$ vertices:**
  
  **Transitive Closure** = $n \times n$ Boolean matrix $T = \{t_{ij}\}$ indicating **reachability**
  
  **Matrix values:**
- $t_{ij} = 1$ if path exists from vertex $i$ to vertex $j$
- $t_{ij} = 0$ if no path exists
  
  **Example:**
  ```
  Graph: 1→2→3
       ↓   ↑
       4→→→5
  
  Closure matrix shows 1 can reach all vertices
  ```
  
  ---
- ### What is Warshall's Principle? #card
  **Big idea:** If path from $A$ to $B$, and path from $B$ to $C$, then path from $A$ to $C$
  
  **Build reachability incrementally:**
  
  Path exists $i \to j$ if:
- Direct edge $i \to j$, OR
- Path through vertex 1, OR
- Path through vertices {1, 2}, OR
- Path through vertices {1, 2, 3}, OR
- ...
- Path through **any subset of vertices**
  
  **Construct matrices:** $R^{(0)}, R^{(1)}, ..., R^{(n)} = T$
  
  Where $R^{(k)}[i,j] = 1$ if path from $i$ to $j$ using only vertices $\{1,...,k\}$ as intermediates
  
  ---
- ### Warshall's Algorithm #card
  ```
  Warshall(AdjacencyMatrix A)
  R^(0) ← A                    // Initialize with direct edges
  
  for k ← 1 to n do            // For each intermediate vertex
    for i ← 1 to n do          // For each source
      for j ← 1 to n do        // For each destination
        R^(k)[i,j] ← R^(k-1)[i,j] OR 
                     (R^(k-1)[i,k] AND R^(k-1)[k,j])
  
  return R^(n)
  ```
  
  **Logic:** $R^{(k)}[i,j] = 1$ if:
- Already had path using vertices $\{1,...,k-1\}$, OR
- Path $i \to k$ AND path $k \to j$ exist
  
  **Efficiency:** $\Theta(n^3)$ - three nested loops
  
  **Space optimization:** Can use single matrix (update in place)
  
  ---
- ### How does Floyd's Algorithm work? #card
  **Problem:** Find **shortest paths** between **all pairs** of vertices
  
  **Similar to Warshall but tracks distances instead of reachability**
  
  **Let** $d_{ij}^{(k)}$ = shortest path from $i$ to $j$ using only vertices $\{1,...,k\}$ as intermediates
  
  **Initialization:**
  $d_{ij}^{(0)} = w_{ij}$
  (Direct edge weight, or $\infty$ if no edge)
  
  **Recurrence:**
  $d_{ij}^{(k)} = \min(d_{ij}^{(k-1)}, d_{ik}^{(k-1)} + d_{kj}^{(k-1)})$
  
  **Meaning:** Shortest path either:
- Doesn't go through $k$: use $d_{ij}^{(k-1)}$
- Goes through $k$: use $d_{ik}^{(k-1)} + d_{kj}^{(k-1)}$
  
  **Termination:** $k = n$ (all vertices considered)
  
  ---
- ### Floyd's Algorithm code #card
  ```
  Floyd(WeightMatrix W)
  D ← W                        // Initialize distances
  
  for k ← 1 to n do            // For each intermediate vertex
    for i ← 1 to n do          // For each source
      for j ← 1 to n do        // For each destination
        D[i,j] ← min(D[i,j], D[i,k] + D[k,j])
  
  return D
  ```
  
  **Efficiency:** $\Theta(n^3)$
  
  **Advantages:**
- Handles negative edge weights (but not negative cycles)
- Simple implementation
- Computes all pairs at once
  
  **To reconstruct paths:** Keep predecessor matrix showing next vertex in shortest path
  
  ---
- ### Dynamic Programming solution for Knapsack Problem #card
  **Subproblem definition:**
- Consider first $i$ items: $1 \leq i \leq n$
- Weights: $w_1, ..., w_i$
- Values: $v_1, ..., v_i$
- Knapsack capacity: $j$ where $0 \leq j \leq W$
  
  **Data structure:** Table $V[i,j]$ where:
- $V[i,j]$ = value of optimal subset of first $i$ items fitting in capacity $j$
  
  **Recurrence:**
  $V[i,j] = \begin{cases}
  0 & \text{if } i = 0 \text{ or } j = 0\\
  V[i-1,j] & \text{if } w_i > j\\
  \max(V[i-1,j], v_i + V[i-1,j-w_i]) & \text{if } w_i \leq j
  \end{cases}$
  
  **Meaning:**
- Can't include item $i$ if too heavy
- Otherwise: max of (exclude item $i$, include item $i$)
  
  ---
- ### Knapsack memory function solution #card
  ```
  Knapsack(i, j, w[], v[], V[][])
  if V[i,j] < 0:               // -1 means not computed yet
    if j < w[i]:
      val ← Knapsack(i-1, j, w, v, V)
    else:
      val ← max(Knapsack(i-1, j, w, v, V),
                v[i] + Knapsack(i-1, j-w[i], w, v, V))
    V[i,j] ← val
  return V[i,j]
  ```
  
  **This is memoization:** Top-down with caching
  
  **Alternative:** Bottom-up table filling (like Coin Collecting)
  
  **Efficiency:** $\Theta(nW)$ - pseudo-polynomial
- Not polynomial in input size (W can be exponentially large in bits)
- Practical when W is reasonable
  
  **Backtracking:** Check if $V[i,j] = V[i-1,j]$ to see if item included
  
  ---
- ## 15. Greedy Algorithms
- ### Principle of Greedy Techniques #card
  **Strategy:** Make sequence of choices that are:
  
  1. **Feasible** - Satisfy problem constraints
  2. **Locally optimal** - Best choice among all feasible options at that step
  3. **Irrevocable** - No backing out / no backtracking
  
  **Hope:** Sequence of locally optimal choices leads to globally optimal solution
  
  **Reality:**
- Doesn't always work
- Sometimes gives approximation (local optimality waived)
- Must prove correctness for each problem
  
  **Example:** Making change with {25, 10, 5, 1} cents
- Greedy works: Always take largest coin possible
- With {25, 10, 1}: Greedy fails for 30 cents (25+1+1+1+1+1 vs 10+10+10)
  
  ---
- ### Greedy algorithm for Change Making #card
  **Algorithm:**
  ```
  GreedyChange(n, D[1..m])  // D sorted descending
  result ← []
  remaining ← n
  for i ← 1 to m do
    while remaining ≥ D[i] do
      result.append(D[i])
      remaining ← remaining - D[i]
  return result
  ```
  
  **At each step:** Choose largest denomination that doesn't exceed remaining amount
  
  **Works optimally for:** "Canonical" coin systems (like US coins)
  
  **Fails for:** Non-canonical systems
- Example: {1, 3, 4} making 6
- Greedy: 4 + 1 + 1 = 3 coins
- Optimal: 3 + 3 = 2 coins
  
  **Efficiency:** $\Theta(n)$ in worst case (all pennies)
  
  ---
- ### What is a Spanning Tree? #card
  **Definition:** For an undirected, connected graph:
  
  **Spanning tree** = Tree that:
- Contains **all vertices** of the graph
- Is **connected** (single component)
- Is **acyclic** (no cycles)
- Has exactly $|V| - 1$ edges
  
  **Example:**
  ```
  Graph:  A---B
        |\ /|
        | X |
        |/ \|
        C---D
  
  Possible spanning trees:
  A---B    A---B
  |   |    |
  C   D    C---D
  ```
  
  ---
- ### What is a Minimum Spanning Tree (MST)? #card
  **Definition:** For weighted graph:
  
  **MST** = Spanning tree with **minimum total edge weight**
  
  **Sum of weights:** $\sum_{e \in \text{MST}} w(e)$ is minimized
  
  **Applications:**
- Network design (minimize cable length)
- Approximation for TSP
- Clustering algorithms
- Image segmentation
  
  **Example:**
  ```
    2     3
  A---B---C
  |   |   |
  4|  1|   |5
  |   |   |
  D---E---F
    6     7
  
  MST uses edges: AB(2), BC(3), BE(1), EF(7) = total 13
  ```
  
  ---
- ### How do we greedily find the MST? #card
  **Two main greedy algorithms:**
  
  **1. Prim's Algorithm** - Grow tree by adding vertices
- Start with arbitrary vertex
- Repeatedly add cheapest edge connecting tree to non-tree vertex
- Complexity: $O(|E| \log |V|)$ with binary heap
  
  **2. Kruskal's Algorithm** - Grow forest by adding edges
- Sort all edges by weight
- Add edges in order if they don't create cycle
- Complexity: $O(|E| \log |E|)$
  
  **Both produce optimal MST!**
  
  **Why greedy works here:** Proven via "cut property" and "cycle property"
  
  ---
- ### What are text encoding requirements? #card
  **Goal:** Assign bit patterns to alphabet characters to minimize total bits used
  
  **Two approaches:**
  
  **1. Fixed length encoding:**
- Same number of bits per character
- Example: ASCII (7-8 bits per character)
- Simple but not space-efficient
  
  **2. Variable length encoding:**
- Bits vary by character frequency
- More frequent characters get shorter codes
- Better compression
- Example: Morse code, Huffman coding
  
  **Key requirement:** Must be **decodable** (uniquely decode bit stream)
  
  ---
- ### How do Huffman Trees relate to text encoding? #card
  **Problem:** How to decode variable-length encoding?
  
  **Solution:** Use **prefix-free codes**
- No code is prefix of another
- Can decode without ambiguity
  
  **Huffman Tree implements prefix-free codes:**
- **Leaves** = characters
- **Left edges** = 0-bit
- **Right edges** = 1-bit
- **Path from root** = code for character
  
  **Example tree:**
  ```
       *
      / \
     0   1
    /     \
   A       *
          / \
         0   1
        /     \
       B       C
  
  Codes: A=0, B=10, C=11
  ```
  
  **Encoding:** "ABACC" → 0100111
  **Decoding:** 0100111 → A|B|A|C|C
  
  ---
- ### Huffman Tree coding algorithm #card
  ```
  HuffmanTree(freq[1..n])
  1. Create n one-node trees (one per character)
  2. Store frequency/weight in each root
  3. Put all trees in priority queue (min-heap by weight)
  
  while queue has more than 1 tree:
    4. Remove two trees with smallest weight
    5. Create new tree with these as left/right subtrees
    6. New root weight = sum of children weights
    7. Add new tree to queue
  
  return remaining tree
  ```
  
  **Example:** Characters {A:5, B:2, C:1, D:1}
  ```
  Step 1: Trees [A:5] [B:2] [C:1] [D:1]
  Step 2: Combine C,D → [A:5] [B:2] [CD:2]
  Step 3: Combine B,CD → [A:5] [BCD:4]
  Step 4: Combine A,BCD → [ABCD:9] ← Final tree
  ```
  
  **Greedy choice:** Always combine two least frequent
  
  ---
- ### Notes on Huffman Coding #card
  **Compression Ratio:**
  $CR = 100 \times \frac{y - x}{y}$
  Where $y$ = uncompressed size, $x$ = compressed size
  
  **Typical:** 20-80% compression with Huffman
  
  **Yields optimal compression when:**
- Character probabilities are independent
- Probabilities known in advance
- Probabilities are powers of 2 (rare in practice)
  
  **Limitations:**
- Not great for binary strings (alphabet = {0,1})
- Needs two passes: frequency counting + encoding
- Must transmit/store tree structure
  
  **Better alternatives:**
- Adaptive Huffman (builds tree while encoding)
- Arithmetic coding
- Modern: LZ77/LZ78, gzip, bzip2
  
  ---
- ## 16. Complexity Analysis - Additional Topics
- ### Checkpoint: True or False questions #card
  
  **Q1:** $\Theta(n + \log n) = \Theta(n)$  
  **A:** TRUE - $\log n$ is dominated by $n$
  
  **Q2:** $O(n + \log n) = O(n)$  
  **A:** TRUE - Same reasoning
  
  **Q3:** $\Theta(n \log_2 n) = \Theta(n \log_{10} n)$  
  **A:** TRUE - $\log_2 n = \frac{\log_{10} n}{\log_{10} 2} \approx 3.32 \log_{10} n$ (constant factor)
  
  **Q4:** $\Theta(\log^2 n) = \Theta(\log n)$  
  **A:** FALSE - $\log^2 n$ grows faster than $\log n$ (not by constant)
  
  **Q5:** $O(n \log n) = O(n)$  
  **A:** FALSE - Different complexity classes, but $O(n) \subset O(n \log n)$
  
  **Q6:** If $x \in O(n \log n)$ then $x \in O(n^2)$  
  **A:** TRUE - $O(n \log n) \subset O(n^2)$
  
  **Q7:** If $x \in \Theta(n \log n)$ then $x \in \Theta(n^2)$  
  **A:** FALSE - $\Theta$ requires exact growth rate
  
  **Q8:** If $x \in O(n \log n)$ then $x \in O(n)$  
  **A:** FALSE - $x$ might be $\Theta(n \log n)$
  
  ---
- ### Analysis Exercise: Mystery Function #card
  ```
  int MysteryFunction(A[0..n-1])
  MysteryVal = A[0]
  for i = 1 to n-1:
    if A[i] > MysteryVal:
      MysteryVal = A[i]
  return MysteryVal
  ```
  
  **What does it do?** Finds maximum element in array
  
  **Basic operation?** Comparison: `A[i] > MysteryVal`
  
  **How many times executed?** $n - 1$ times
  
  **Asymptotic efficiency?** $\Theta(n)$
  
  ---
- ### Analysis Exercise: Is it a Set? #card
  **Problem:** Check if array has duplicates (i.e., is it a set?)
  
  **Brute Force:** Compare every pair
  ```
  boolean isset(A):
  for i = 0 to A.length-1:
    for j = 0 to A.length-1:
      if i ≠ j and A[i] = A[j]:
        return false
  return true
  ```
  **Efficiency:** $\Theta(n^2)$ - $(n-1)^2$ comparisons
  
  **Improved Brute Force:** Only compare each pair once
  ```
  boolean isset(A):
  for i = 0 to A.length-2:
    for j = i+1 to A.length-1:
      if A[i] = A[j]:
        return false
  return true
  ```
  **Efficiency:** $\Theta(n^2)$ - $\frac{n(n-1)}{2}$ comparisons
  
  **Transform & Conquer:** Sort then scan
  ```
  boolean isset(A):
  sort(A)                        // Θ(n log n)
  for i = 0 to A.length-2:       // Θ(n)
    if A[i] = A[i+1]:
      return false
  return true
  ```
  **Efficiency:** $\Theta(n \log n)$ - Much better!
  
  ---
- ### Brute Force Convex Hull #card
  **Problem:** Find convex hull of $n$ points in 2D
  
  **Definition:** Smallest convex set containing all points
  
  **Brute Force Algorithm:**
  ```
  For each pair of points (p1, p2):
  Draw line through p1 and p2
  Check if all other points on same side
  If yes: edge is part of hull
  ```
  
  **How to check "same side":** Use cross product sign
  
  **Efficiency:** $\Theta(n^3)$
- $\binom{n}{2} = \frac{n(n-1)}{2}$ pairs
- Check $n-2$ other points for each pair
- Total: $\frac{n(n-1)(n-2)}{2} \in \Theta(n^3)$
  
  **Better algorithms:** QuickHull $O(n \log n)$ average, Graham Scan $O(n \log n)$
  
  ---
- ### String Matching Worst Case Analysis #card
  **Brute Force String Matching:**
  ```
  Text: n characters
  Pattern: m characters
  ```
  
  **Worst case scenario:**
- Pattern matches on every character except last
- For every position in text
  
  **Example:**
  ```
  Text:    "aaaaaaaaaaaaaaaa..." (n chars)
  Pattern: "aaab" (m chars)
  ```
  
  **Analysis:**
- Outer loop: $n - m + 1$ iterations
- Inner loop: $m$ comparisons (worst case)
- Total: $m(n - m + 1) \in \Theta(mn)$ when $m \ll n$
  
  **Average case:** $\Theta(n)$ for natural language (early mismatches common)
  
  ---
- ## 17. Final Review Topics
- ### Summary: Asymptotic Notation #card
  
  | Notation | Meaning | Bound Type |
  |----------|---------|------------|
  | Big O | At most | Upper bound (≤) |
  | Big Ω | At least | Lower bound (≥) |
  | Big Θ | Exactly | Tight bound (=) |
  
  **Examples:**
- Average of $O(n^2)$: grows at most as $n^2$ on average
- Worst case of $\Omega(2^n)$: grows at least as $2^n$ in worst case
- Best case of $\Theta(n)$: grows exactly as $n$ in best case
  
  **Common complexities (slow → fast):**
  $\Theta(1) < \Theta(\log n) < \Theta(n) < \Theta(n \log n) < \Theta(n^2) < \Theta(2^n) < \Theta(n!)$
  
  ---
- ### Algorithm Design Strategies Summary #card
  
  | Strategy | When to Use | Example |
  |----------|-------------|---------|
  | Brute Force | Small inputs, baseline | Exhaustive search |
  | Decrease & Conquer | Can reduce problem size | Binary search, Insertion sort |
  | Divide & Conquer | Independent subproblems | MergeSort, QuickSort |
  | Transform & Conquer | Different representation helps | Presorting, Heaps |
  | Dynamic Programming | Overlapping subproblems | Knapsack, Floyd's |
  | Greedy | Local optimality works | Huffman, MST |
  
  ---
- ### When to Use Which Algorithm? #card
  
  **Sorting:**
- Small arrays: Insertion Sort $\Theta(n^2)$
- General purpose: QuickSort $O(n \log n)$ avg
- Guaranteed time: MergeSort $\Theta(n \log n)$
- Nearly sorted: Insertion Sort $O(n)$ best
- Restricted range: Counting Sort $\Theta(n+k)$
  
  **Searching:**
- Unsorted: Linear $\Theta(n)$
- Sorted: Binary $\Theta(\log n)$
- Uniform distribution: Interpolation $O(\log \log n)$
- kth element: QuickSelect $O(n)$ avg
  
  **Graphs:**
- All-pairs shortest: Floyd $\Theta(n^3)$
- Single-source: Dijkstra $O((V+E) \log V)$
- Transitive closure: Warshall $\Theta(n^3)$
- MST: Prim/Kruskal $O(E \log V)$
  
  **String Matching:**
- Simple: Brute Force $O(mn)$
- Practical: Boyer-Moore / Horspool
- Guaranteed: KMP $\Theta(m+n)$