- What are the coding principles? #card
	- DRY
	- OneTask
	- Meaningful Names
	- No littering
	- No magic constants
	- Const correctness
- Describe the DRY principle #card
	- Dont repeat yourself: prefer to move logic to functions
- Describe the OneTask Principle #card
	- Do one thing in a function
- How should meaningful names be given? #card
	- Functions: verbNoun
	- Variables: two or more nouns
	- Long scope => Long name
	- Limit the lifetime of variables
	- Dont yse Hungarian notation
	- Use consistent names
- Describe the No littering principle #card
	- Code is always up to date
		- So try put descriptions in the code, less in comments
	- Prefer versioning system over commenting out code
	- Comments should explain why, not what
- Describe the no magic constants principle #card
	- name you constants so intention is clear
		- Easier to document and all in one place (DRY)
- Describe the const correctness principle #card
	- Use constants
		- Easier to use pure functions that are predictable
- Describe the visitor principle #card
	- Create a visitor class that operates on behalf of the classes
	- Allows for separation of concerns
- What are the pros and cons of the visitor pattern #card
	- Pros
		- Easily add functionality
		- Separate concerns
	- Cons
		- Adding new types means updating all visitors
		- Limits operations to what the objects make public
		- Requires a god object to handle many tasks
		- Heap allocation for unknown types
		- Each element needs to duplicate a visit_me function to call on the visitor
		- Even using matches against the visitor is difficult to maintain
- What are the 3 types of parallel programming? #card
	- Task
	- Data
	- Instruction
- Describe task parallelism #card
	- Concurrent programs
	- Many data structures
	- Data races as you mutate the same data differently
- Describe Data parallelism #card
	- Synchronised, doing work on its own data
	- One task on many data
	- Or multiple tasks on many data
	- Not many tasks on one data
	- No races
- Describe instruction parallelism #card
	- Micro optimisations
		- Pipelining
		- Superscalar
	- Automatic on CPU
	- Bit parallelism
	- Manual on GPU
	- Allows for Out of order execution and branch predicting
- What does pipelining allow the CPU to achieve? #card
	- Out of order execution
	- Speculative execution
	- Branch prediction
	- Cache prediction
- Discuss bit parallelism hand how larger words aid #card
	- Bit parallelism for small word machines is difficult and more tedious as you need to explicitly handle carries
	- For larger words, they automatically carry over for you
		- But, if you want to store more independent data in the large word, small words are easier as they do not propagate the carry over to the next word
- Discuss pipelining in parallelism #card
	- an instruction has multiple stages
	- The CPU can handle these various stages in parallel per cycle.
		- this means that instructions can be carried out stagnated, where instruction 2 is loaded and ready to execute while instruction 3 is *being* loaded
	- Describe what happens in a pipeline stall #card
		- When an instruction needs to execute but relies on a previous instruction to complete
	- Describe a pipeline flush #card
		- When a CPU realises that an instruction in a pipeline should not execute (often cause the assumption or condition has been fetched and checked, while other lower instructions are ready to operate on that condition)
	- What is Out of order execution and how does it work? #card
		- The CPU creates a dependency graph to ensure that the instruction it is about to load does not rely on a value or instruction that has not been executed.
		- This allows for execution to skip some parts of code and run stuff that does not need the previous execution orders
- What is Superscalar #card
	- The use of executing multiple pipeline stages per cycle by using multiple execution resources for each pipeine stage.
		- This means that you can fetch multiple instructions at once
- What are the steps in SuperScalar or Out of Order execution? #card
	- instructions are fetched and decoded in order
	  logseq.order-list-type:: number
	- Dependency graph is built
	  logseq.order-list-type:: number
	- Instructions are reordered
	  logseq.order-list-type:: number
	- Assigned to execution units/resources
	  logseq.order-list-type:: number
	- Executed out of order
	  logseq.order-list-type:: number
	- Store results in buffer
	  logseq.order-list-type:: number
	- Reorder the output
	  logseq.order-list-type:: number
	- Move buffer to cache
	  logseq.order-list-type:: number
- Why delegate Out of order execution to the CPU? #card
	- Compiler does not have runtime access
	- Compiler can only reorder once
- What is speculative execution and what problem does it solve? #card
	- The CPU guesses which branch is correct and executes the block inside the branch before the branch is confirmed to be correct.
	- This is supposed to maximise the instructions per cycle
- What are the dangers of speculative execution? #card
	- Stalls are the same but mispredictions are costly
		- Dependecy graph must be rebuilt
		- Redundant work is done
- What are considerations taken into account with speculative execution? #card
	- Execution output (for out of order execution and speculative execution) is stored in a buffer and not committed anywhere
		- This makes executions rewindable
- What are security risks of speculative execution #card
	- Side channel essentially are ways to "read into" a block of preemptivly executed blocks by observing the side-effects of an execution (time taken. cache used up etc.)
- What are the performance differences between CPUs and GPUs #card
	- CPU: Low latency and low Thtoughput
	- GPU: High latency and high throughput
		- GPUs dont have optimisations like pipelining, superscalar, branch prediction, prefetching and speculative or out of order execution
- Explain SIMD #card
	- Single instruction multiple data
		- This is when the same instruction can be executed over different data in parallel.
			- This is more efficient on memory
			- You get more work per cycle
	- What is vectorisation? #card
		- The ability to tell the cpu what instructions to run on a vector and the CPU will execute that instruction using superscalar
		- Best used when Performance is critical and algorithms are optimal
- What is task parallelism? #card
	- Multiple programs where each task has it's own data
		- This leads to a lot of memory being used
	- Communication between tasks introduces overhead
	- Difficult to prove correctness
- What are the trade offs between distributed and Shared memory? #card
	- Distributed: Correct but slow
	- Shared: Fast but complex
- How is shared memory often implemented? #card
	- Using data structures that are often ordered
		- Trees
		- Queues
		- Stacks
	- Using locks
		- Locked: One task is working on that data
			- Easier for correctness but
			- Idle waiting
			- What are the drawbacks of using a single Locking mechanism? #card
				- Deadlock
				- Starvation
			- What guarantees does a single lock mechanism have? #card
				- There are none and a risk of deadlock by 1 thread
		- Lock free: Task only locks what they need
			- Allows for multiple work/ reduces idle waiting
			- Difficult to coordinate effectively
			- What are the drawbacks and benefits of Lock-free mechanism? #card
				- One task will always make progress
				- Starvation or livelocking
			- What guarantees does Lock free mechanism provide? #card
				- No thread can lock the system, but Live locks can occur
		- Wait free: Reduce contention by helping the locker finish their work
			- Guarantees a fixed number of execution steps by telling all later threads what work needs to be done.
				- Prevents idle waiting
		-
- Describe Aspect Oriented programming #card
	- A mechanism to add functionality to existing business logic that is hidden and separates the business logic from the added injection
- Why would aspect oriented programming me implemented? #card
	- Retroactively adding new code that must be added to multiple functions
	- When refactor is crazy complex
	- Where there is a cross cutting concern that needs to be added
		- Different methods/ classes need the same intervention
	- What are the terms used in AOP? #card
		- Advice
			- The piece of code that needs to be injected
		- Pointcut
			- Location to inject advice:
				- Before
				- After
				- Around
				- After exception
				- After return
			- Defined using regex
		- Aspect
			- Advice + pointcut
- How does Aspect oriented programming work? #card
	- Reflection definition #card
		- The ability of a process to examine and modify it's own behaviour
	- Dynamic reflection definition #card
		- Reflection at runtime
	- Static reflection #card
		- Reflection at compile time