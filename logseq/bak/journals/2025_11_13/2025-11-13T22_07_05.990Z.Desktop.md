# **Computability & Complexity — Expanded Study Notes (with Flash Cards)**

These notes walk through the material **as a continuous conceptual narrative**, but also include **granular flash cards** for memorisation.
Use the textbook flow for understanding and the `` blocks for mastery. #card

---
- # **1. Foundations of Computation**
  
  Computability theory begins with a central question:
  
  > **What does it mean for a function or problem to be computable?**
  
  Before digital computers existed, Turing answered this precisely by defining an abstract model that captures *all mechanically performable computations*. This model is the **Turing Machine (TM)**.
  
  ---
- ## **1.1 Turing Machines**
  
  Turing Machines model computation using extremely simple components: a tape, a head, states, and transitions. Despite their simplicity, they capture everything a modern computer can compute (Church–Turing thesis).
- ### **1.1.1 Formal Structure**
- Turing machine — definition #card
	- A **Turing machine (TM)** consists of:
	  
	  * An infinite tape divided into cells
	  * A tape alphabet (includes a blank symbol `◊`)
	  * A tape head that reads/writes a symbol and moves L/R
	  * A finite set of states (including an initial and halting states)
	  * A transition function δ: (state, symbol) → (state, symbol, direction)
	  
	  This formalizes the notion of an *algorithmic step*.
	  
	  ---
- ## **1.2 How a Turing Machine Computes**
- A TM operates by repeatedly:
- 1. Reading the symbol under the head
  2. Determining the transition rule
  3. Writing a symbol
  4. Moving left or right
  5. Entering a new state
- The entire **instantaneous state of the machine**—tape content, head position, and control state—is called a **configuration**.
  
  TM configuration example #card
  If configuration is `1011q701111`, the head is at the digit immediately after `q7`.
  If δ(q7,0) = (q2,R), then next configuration is `10110q21111`.
  
  ---
- ## **1.3 Halting, Crashing, or Looping**
  
  A TM execution may:
  
  1. **Halt normally** in a halt state
  2. **Crash** due to undefined transitions
  3. **Loop forever**, never halting
  
  The looping scenario becomes crucial later when we study *undecidability*.
  
  TM halting #card
  A machine **halts** only when it enters a halting state. If it never reaches such a state, we say it *loops*.
  
  ---
- ## **1.4 Transition Diagrams & How to Read Them**
  
  Transition diagrams summarize δ visually. An arc labelled:
  
  * `a/b,R` means: read `a`, write `b`, move right
  * `a/a` means: read `a`, write `a` (no change)
  
  These provide an intuitive way to design TMs for tasks such as incrementing numbers, copying data, shifting strings, etc.
  
  ---
- ## **1.5 Designing TMs Using Building Blocks**
  
  Because full TM tables are tedious, we often work with small **building blocks** (e.g., “move right until blank”, “shift right”, “find next 1”).
  These can be composed to build large machines.
  
  TM modular design #card
  To combine machines M1 → M2, connect halt states of M1 to start state of M2.
  Useful when constructing multi-stage algorithms.
  
  ---
- ## **1.6 Multi-Tape and Nondeterministic Turing Machines**
  
  We can extend the TM model to allow:
  
  * **Multiple tapes** (one head each), or
  * **Nondeterministic branching**
  
  These models do **not** increase what is computable—only how efficiently.
  
  Multi-tape TM equivalence #card
  Any multi-tape TM can be simulated by a single-tape TM with polynomial overhead.
  
  Nondeterministic TM equivalence #card
  NTMs recognize the same class of languages as deterministic TMs, but may run faster.
  
  This equivalence leads directly into complexity theory (P vs NP), but first we need a stronger foundation about *languages*.
  
  ---
- # **2. Describing Languages: Regular Expressions & Beyond**
  
  Before studying what problems TMs can or cannot solve, we need a language framework.
  
  ---
- ## **2.1 Regular Expressions (RegExps)**
  
  Regular expressions describe sets of strings through operators:
  
  * **Concatenation**
  * **Alternation** `|`
  * **Kleene star** `*`
  * **Grouping** with parentheses
  
  Regex basics #card
  Examples:
  
  * `(a|b)(a|b)` = all binary strings of length 2
  * `(ab)*` = ε, `ab`, `abab`, ...
  * `a|a*b` = either `a` OR any number of `a`’s followed by `b`
  
  ---
- ## **2.2 Why Regular Expressions Matter**
  
  While RegExps describe only *regular* languages, Turing Machines describe *all computable* languages.
  Understanding RegExps helps understand *finite* computation, before stepping into *infinite-tape* computation.
  
  ---
- # **3. Computability Theory**
  
  Now that we have the TM model and a language framework, we can ask:
  
  > **Which problems can Turing machines solve?
  > Which problems can *no* machine ever solve?**
  
  This leads to concepts of **decidability**, **recognisability**, and most famously the **Halting Problem**.
  
  ---
- ## **3.1 Decidable vs Acceptable (Recognisable) Languages**
  
  Acceptable vs Decidable #card
  
  * A language is **decidable** if a TM halts on all inputs (accept or reject).
  * A language is **acceptable / RE** if a TM halts only on inputs *in* the language; may loop otherwise.
  
  **All decidable languages are acceptable, but not vice-versa.**
  
  ---
- ## **3.2 Encoding TMs as Numbers**
  
  Since a TM is a finite table of rules, we can encode it as binary.
  This lets us treat TMs as *data*, so a TM can process the code of another TM.
  
  This leads directly to **self-reference** and is essential for proving the **Halting Problem**.
  
  TM encoding #card
  Assign binary codes to states, directions, and tape symbols.
  Concatenate them → *Turing number*.
  
  ---
- ## **3.3 Countability Argument**
  
  There are **countably many Turing machines** (like integers).
  But there are **uncountably many languages** (like real numbers).
  
  Thus, many languages cannot be recognized or decided by *any* TM.
  
  Countability #card
  ∣TMs∣ = countable
  ∣Languages∣ = uncountable
  ⇒ Some languages are uncomputable.
  
  ---
- ## **3.4 The Halting Problem**
  
  This is the most fundamental undecidable problem.
  
  Halting problem #card
  Given TM encoding `Ti`, does `Mi` halt on input `Ti`?
  No TM can always decide this.
- ### **Proof Idea (Diagonalization)**
  
  Assume a decider `H` exists.
  Construct TM `W` that:
  
  * halts if H says “no”
  * loops if H says “yes”
  
  Then ask what `W(W)` does. Contradiction.
  
  Halting proof sketch #card
  Self-reference creates a paradox: W halts iff W loops.
  
  ---
- ## **3.5 Implications of Undecidability**
  
  * No general program can predict whether arbitrary programs halt.
  * No algorithm exists for many software analysis tasks.
  * Reductions allow showing other problems are undecidable.
  
  ---
- # **4. Complexity Theory**
  
  After decidability, we ask:
  
  > **For problems *that are computable*, how much time do they require?**
  
  This leads to **complexity classes**.
  
  ---
- ## **4.1 Class P**
  
  Class P #card
  P = decision problems solvable by deterministic TMs in polynomial time.
  
  These are the problems we consider “efficiently solvable”.
  
  ---
- ## **4.2 Class NP**
  
  Class NP #card
  NP = problems whose solutions can be verified in polynomial time.
  Equivalently: problems solvable by an NTM in polynomial time.
- ### Certificates
  
  For NP problems, a “certificate” is a guessed solution that is easy to check.
  
  ---
- ## **4.3 NP-Complete Problems**
  
  These are the **hardest** problems in NP.
  
  NP-complete definition #card
  A problem X is NP-complete if:
  
  1. X ∈ NP
  2. Every NP problem reduces to X in polynomial time
  
  Cook’s Theorem: SAT is NP-complete.
  
  ---
- ## **4.4 Reductions**
  
  A reduction converts instances of one problem into instances of another.
  
  Reduction HP→TSP example #card
  To reduce Hamiltonian Path to TSP, assign low cost to original edges and high cost to absent edges.
  A Hamiltonian path exists ⇔ a low-cost TSP tour exists.
  
  ---
- ## **4.5 Examples of NP-complete problems**
  
  * SAT
  * 3SAT
  * Clique
  * Vertex Cover
  * Hamiltonian Path/Cycle
  * TSP decision version
  * Subset Sum
  
  ---
- # **5. Algorithmic Lower Bounds**
  
  Lower bounds tell us **how hard** a problem must be.
  
  ---
- ## **5.1 Trivial Lower Bounds**
  
  Trivial lower bound #card
  Lower bound from input/output size.
  Example: generating all permutations has Ω(n!) time because output has n! items.
  
  ---
- ## **5.2 Adversary Methods**
  
  Adversary argument #card
  An adversary answers queries in the worst consistent manner to force maximum work.
  Example: finding max among n requires at least n−1 comparisons.
  
  ---
- # **6. Strategies for Hard Problems**
  
  When a problem is provably or likely intractable, we use refined algorithmic techniques.
  
  ---
- ## **6.1 Backtracking**
  
  Explores state-space tree depth-first, pruning invalid branches.
  
  Backtracking #card
  Used for feasibility problems (e.g., n-queens).
  At each node, check constraints; backtrack on failure.
  
  ---
- ## **6.2 Branch and Bound**
  
  Best-first variant of backtracking for optimization problems.
  
  Branch and bound #card
  Use bounds (lower or upper) to prune branches that cannot lead to a better solution.
  
  Example: Assignment problem, TSP with lower bound pruning.
  
  ---
- # **7. Worked TM Constructions**
  
  ---
- ## **7.1 Unary Increment TM**
  
  Unary increment TM #card
  Algorithm:
  
  1. Move right over 1’s to the blank.
  2. Write `1` to extend unary number.
  3. Return head to start position.
  
  ---
- ## **7.2 Binary Increment (Reversed Representation)**
  
  Binary reversed increment TM #card
  Scan left-to-right:
  
  * If 0 encountered: flip to 1 → done
  * If 1 encountered: flip to 0, continue
  * If blank encountered: write 1 (carry overflow)
  
  ---
- # **8. Exam Strategy Notes**
  
  ---
  
  Exam strategy #card
  
  * Always write TMs clearly: show state marker before scanned symbol
  * Use building blocks for TM design
  * For reductions: prove correctness in both directions
  * State polynomial bounds explicitly
  * For undecidability proofs: always use self-reference carefully
  Below is a **fully expanded, hierarchical, textbook-style version** of your notes, integrating **all the flash cards** and **adding more explanatory flow** between topics.
  Each atomic concept is still a ``, but now embedded inside properly structured sections so the material reads like a complete set of study notes, not disjoint cards. #card
  
  I preserved accuracy to your slides and added examples, motivations, and intuitive bridges between topics.
  If you want, I can also output a *separate clean file* containing only cards, only textbook sections, or both merged.
  
  ---
- # **Computability & Complexity — Expanded Study Notes (with Flash Cards)**
  
  These notes walk through the material **as a continuous conceptual narrative**, but also include **granular flash cards** for memorisation.
  Use the textbook flow for understanding and the `` blocks for mastery. #card
  
  ---
- # **1. Foundations of Computation**
  
  Computability theory begins with a central question:
  
  > **What does it mean for a function or problem to be computable?**
  
  Before digital computers existed, Turing answered this precisely by defining an abstract model that captures *all mechanically performable computations*. This model is the **Turing Machine (TM)**.
  
  ---
- ## **1.1 Turing Machines**
  
  Turing Machines model computation using extremely simple components: a tape, a head, states, and transitions. Despite their simplicity, they capture everything a modern computer can compute (Church–Turing thesis).
- ### **1.1.1 Formal Structure**
  
  Turing machine — definition #card
  A **Turing machine (TM)** consists of:
  
  * An infinite tape divided into cells
  * A tape alphabet (includes a blank symbol `◊`)
  * A tape head that reads/writes a symbol and moves L/R
  * A finite set of states (including an initial and halting states)
  * A transition function δ: (state, symbol) → (state, symbol, direction)
  
  This formalizes the notion of an *algorithmic step*.
  
  ---
- ## **1.2 How a Turing Machine Computes**
  
  A TM operates by repeatedly:
  
  1. Reading the symbol under the head
  2. Determining the transition rule
  3. Writing a symbol
  4. Moving left or right
  5. Entering a new state
  
  The entire **instantaneous state of the machine**—tape content, head position, and control state—is called a **configuration**.
  
  TM configuration example #card
  If configuration is `1011q701111`, the head is at the digit immediately after `q7`.
  If δ(q7,0) = (q2,R), then next configuration is `10110q21111`.
  
  ---
- ## **1.3 Halting, Crashing, or Looping**
  
  A TM execution may:
  
  1. **Halt normally** in a halt state
  2. **Crash** due to undefined transitions
  3. **Loop forever**, never halting
  
  The looping scenario becomes crucial later when we study *undecidability*.
  
  TM halting #card
  A machine **halts** only when it enters a halting state. If it never reaches such a state, we say it *loops*.
  
  ---
- ## **1.4 Transition Diagrams & How to Read Them**
  
  Transition diagrams summarize δ visually. An arc labelled:
  
  * `a/b,R` means: read `a`, write `b`, move right
  * `a/a` means: read `a`, write `a` (no change)
  
  These provide an intuitive way to design TMs for tasks such as incrementing numbers, copying data, shifting strings, etc.
  
  ---
- ## **1.5 Designing TMs Using Building Blocks**
  
  Because full TM tables are tedious, we often work with small **building blocks** (e.g., “move right until blank”, “shift right”, “find next 1”).
  These can be composed to build large machines.
  
  TM modular design #card
  To combine machines M1 → M2, connect halt states of M1 to start state of M2.
  Useful when constructing multi-stage algorithms.
  
  ---
- ## **1.6 Multi-Tape and Nondeterministic Turing Machines**
  
  We can extend the TM model to allow:
  
  * **Multiple tapes** (one head each), or
  * **Nondeterministic branching**
  
  These models do **not** increase what is computable—only how efficiently.
  
  Multi-tape TM equivalence #card
  Any multi-tape TM can be simulated by a single-tape TM with polynomial overhead.
  
  Nondeterministic TM equivalence #card
  NTMs recognize the same class of languages as deterministic TMs, but may run faster.
  
  This equivalence leads directly into complexity theory (P vs NP), but first we need a stronger foundation about *languages*.
  
  ---
- # **2. Describing Languages: Regular Expressions & Beyond**
  
  Before studying what problems TMs can or cannot solve, we need a language framework.
  
  ---
- ## **2.1 Regular Expressions (RegExps)**
  
  Regular expressions describe sets of strings through operators:
  
  * **Concatenation**
  * **Alternation** `|`
  * **Kleene star** `*`
  * **Grouping** with parentheses
  
  Regex basics #card
  Examples:
  
  * `(a|b)(a|b)` = all binary strings of length 2
  * `(ab)*` = ε, `ab`, `abab`, ...
  * `a|a*b` = either `a` OR any number of `a`’s followed by `b`
  
  ---
- ## **2.2 Why Regular Expressions Matter**
  
  While RegExps describe only *regular* languages, Turing Machines describe *all computable* languages.
  Understanding RegExps helps understand *finite* computation, before stepping into *infinite-tape* computation.
  
  ---
- # **3. Computability Theory**
  
  Now that we have the TM model and a language framework, we can ask:
  
  > **Which problems can Turing machines solve?
  > Which problems can *no* machine ever solve?**
  
  This leads to concepts of **decidability**, **recognisability**, and most famously the **Halting Problem**.
  
  ---
- ## **3.1 Decidable vs Acceptable (Recognisable) Languages**
  
  Acceptable vs Decidable #card
  
  * A language is **decidable** if a TM halts on all inputs (accept or reject).
  * A language is **acceptable / RE** if a TM halts only on inputs *in* the language; may loop otherwise.
  
  **All decidable languages are acceptable, but not vice-versa.**
  
  ---
- ## **3.2 Encoding TMs as Numbers**
  
  Since a TM is a finite table of rules, we can encode it as binary.
  This lets us treat TMs as *data*, so a TM can process the code of another TM.
  
  This leads directly to **self-reference** and is essential for proving the **Halting Problem**.
  
  TM encoding #card
  Assign binary codes to states, directions, and tape symbols.
  Concatenate them → *Turing number*.
  
  ---
- ## **3.3 Countability Argument**
  
  There are **countably many Turing machines** (like integers).
  But there are **uncountably many languages** (like real numbers).
  
  Thus, many languages cannot be recognized or decided by *any* TM.
  
  Countability #card
  ∣TMs∣ = countable
  ∣Languages∣ = uncountable
  ⇒ Some languages are uncomputable.
  
  ---
- ## **3.4 The Halting Problem**
  
  This is the most fundamental undecidable problem.
  
  Halting problem #card
  Given TM encoding `Ti`, does `Mi` halt on input `Ti`?
  No TM can always decide this.
- ### **Proof Idea (Diagonalization)**
  
  Assume a decider `H` exists.
  Construct TM `W` that:
  
  * halts if H says “no”
  * loops if H says “yes”
  
  Then ask what `W(W)` does. Contradiction.
  
  Halting proof sketch #card
  Self-reference creates a paradox: W halts iff W loops.
  
  ---
- ## **3.5 Implications of Undecidability**
  
  * No general program can predict whether arbitrary programs halt.
  * No algorithm exists for many software analysis tasks.
  * Reductions allow showing other problems are undecidable.
  
  ---
- # **4. Complexity Theory**
  
  After decidability, we ask:
  
  > **For problems *that are computable*, how much time do they require?**
  
  This leads to **complexity classes**.
  
  ---
- ## **4.1 Class P**
  
  Class P #card
  P = decision problems solvable by deterministic TMs in polynomial time.
  
  These are the problems we consider “efficiently solvable”.
  
  ---
- ## **4.2 Class NP**
  
  Class NP #card
  NP = problems whose solutions can be verified in polynomial time.
  Equivalently: problems solvable by an NTM in polynomial time.
- ### Certificates
  
  For NP problems, a “certificate” is a guessed solution that is easy to check.
  
  ---
- ## **4.3 NP-Complete Problems**
  
  These are the **hardest** problems in NP.
  
  NP-complete definition #card
  A problem X is NP-complete if:
  
  1. X ∈ NP
  2. Every NP problem reduces to X in polynomial time
  
  Cook’s Theorem: SAT is NP-complete.
  
  ---
- ## **4.4 Reductions**
  
  A reduction converts instances of one problem into instances of another.
  
  Reduction HP→TSP example #card
  To reduce Hamiltonian Path to TSP, assign low cost to original edges and high cost to absent edges.
  A Hamiltonian path exists ⇔ a low-cost TSP tour exists.
  
  ---
- ## **4.5 Examples of NP-complete problems**
  
  * SAT
  * 3SAT
  * Clique
  * Vertex Cover
  * Hamiltonian Path/Cycle
  * TSP decision version
  * Subset Sum
  
  ---
- # **5. Algorithmic Lower Bounds**
  
  Lower bounds tell us **how hard** a problem must be.
  
  ---
- ## **5.1 Trivial Lower Bounds**
  
  Trivial lower bound #card
  Lower bound from input/output size.
  Example: generating all permutations has Ω(n!) time because output has n! items.
  
  ---
- ## **5.2 Adversary Methods**
  
  Adversary argument #card
  An adversary answers queries in the worst consistent manner to force maximum work.
  Example: finding max among n requires at least n−1 comparisons.
  
  ---
- # **6. Strategies for Hard Problems**
  
  When a problem is provably or likely intractable, we use refined algorithmic techniques.
  
  ---
- ## **6.1 Backtracking**
  
  Explores state-space tree depth-first, pruning invalid branches.
  
  Backtracking #card
  Used for feasibility problems (e.g., n-queens).
  At each node, check constraints; backtrack on failure.
  
  ---
- ## **6.2 Branch and Bound**
  
  Best-first variant of backtracking for optimization problems.
  
  Branch and bound #card
  Use bounds (lower or upper) to prune branches that cannot lead to a better solution.
  
  Example: Assignment problem, TSP with lower bound pruning.
  
  ---
- # **7. Worked TM Constructions**
  
  ---
- ## **7.1 Unary Increment TM**
  
  Unary increment TM #card
  Algorithm:
  
  1. Move right over 1’s to the blank.
  2. Write `1` to extend unary number.
  3. Return head to start position.
  
  ---
- ## **7.2 Binary Increment (Reversed Representation)**
  
  Binary reversed increment TM #card
  Scan left-to-right:
  
  * If 0 encountered: flip to 1 → done
  * If 1 encountered: flip to 0, continue
  * If blank encountered: write 1 (carry overflow)
  
  ---
- # **8. Exam Strategy Notes**
  
  ---
  
  Exam strategy #card
  
  * Always write TMs clearly: show state marker before scanned symbol
  * Use building blocks for TM design
  * For reductions: prove correctness in both directions
  * State polynomial bounds explicitly
  * For undecidability proofs: always use self-reference carefully
