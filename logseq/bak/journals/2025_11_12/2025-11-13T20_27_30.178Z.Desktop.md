- What are algorithms? #card
	- *Sequence* of *unambiguous instructions*  for solving a *well-defined* problem
- What are the defining features of Algorithms? #card
	- Finite
		- Terminates after a finite number of steps
	- Definite
		- Rigorously and unambiguously specified
	- Input
		- Valid inputs are clearly specified
	- Output
		- Can be proved to produce the correct output given a valid input
	- Effective
		- Steps are sufficiently simple and basic
- Additional, important notions about Algorithms #card
	- Each step must be unambiguous
	- The *range* of inputs must be specified clearly
	- The same algorithm can be represented in different ways
	- Several algorithms for solving the same problem may exist with different properties
- When is it pointless finding the fastest algorithm? #card
	- The system is not bottlenecked at algorithm target
	- The program is not run often
	- Time is not a major issue
- Algorithm design basics #card
	- Specify algorithm
		- Often in Pseudocode
	- Take note of data structures
	- Design principles:
		- Simple
		- General
			- Range of inputs
		- Optimal
			- No algorithm can do this better
	- Formal algorithm analysis looks at Efficiency in time and space
- Fibonacci Piecewise function #card
	- $$
	  F(n) = 
	  \begin{cases}
	    0 & \text{if } n = 0 \\
	    1   & \text{if } n = 1 \\
	    F(n-1) + F(n-2)   & \text{if } n > 1
	  \end{cases}
	  $$
- Typical Problem Domains #card
	- Sorting and searching
	- String processing
	- Graph problems
	- Combinatorial Problems
	- Geometric Problems
	- Numerical Problems
- Types of Algorithms #card
	- Brute Force
		- Try all possibilities
	- Decrease and conquer
		- Solve larger in terms of smaller instances
	- Divide and Conquer
		- Break problems into distinct subproblems
	- Transformation
		- Convert one problem into another
	- Space/Time Trading
		- Use additional data structures
	- Dynamic programming
		- Break problems into overlapping subproblems
	- Greedy
		- Repeatedly do whats best for now
- Naive Brute force Fibionacci (1) #card
	- ```
	  function fibo1(n):
	  if n == 0: return 0
	  else if n == 1: return 1
	  return fibo(n - 1) + fibo(n - 2)
	  ```
- How long does this algorithm take?
  ```
  function fibo1(n):
  if n == 0: return 0
  else if n == 1: return 1
  return fibo(n - 1) + fibo(n - 2)
  ```
  #card
	- $$
	  \text {let T(n) = number of computer steps } \\
	  \text {let n = input value for fibo }
	  $$
	- $$
	  T(n) \le 2 \text{ for } n \le 1
	  $$
	- $$
	  T(n) = T(n - 1) + T(n - 2) + 3 \text { for } n \le 1\\
	  \text {3 is because: }\\ 
	  \text {First condition checking if n is 0} \\ 
	  \text {+ second condition for n = 1} \\ 
	  \text {and final operation adding the child values}
	  $$
-
- Naive Brute force Fibonacci (2) #card
	- ```
	  function fibo2(n):
	  if n = 0: return 0
	  let array = [size of n]
	  f[0] = 0
	  f[1] = 1
	  for i in 2..n:
	  	array[i] = fibo2(i - 1) + fibo2(i - 2)
	  return f[n]
	  ```
- Why do we leave out lower order terms when calculating basic computer steps? #card
	- The high order term will overpower the lower terms significantly as $n$ grows
- Define Big O functions #card
	- $$
	  \text {function }f=O(g) \\
	  \text {if there exists a constant }c > 0 \\
	  \text {and a non-negative integer } n_0 \\
	  \text {such that }f(n) \le c \times g(n) \\
	  \text {for all } n \ge n_0
	  $$
- Define Big $\Omega$ functions #card
	- $$
	  \text {function } f = \Omega(g) \\
	  \text {if there exists a contant } c > 0 \\
	  \text {and a non-negative integer } n_0 \\
	  \text {such that } f(n) \ge c\times g(n) \\
	  \text {for all } n \ge n_0
	  $$
- Define Big $\Theta$ functions #card
	- $$
	  \text {function } f = \Theta(g) \\
	  \text {if there exists some constants } c_0, c_1 \\
	  \text {and a non-negative integer } n_0 \\
	  \text {such that }f(n) \le c_0 \times g(n) \\
	  \text {and }f(n) \ge c_1 \times g(n)
	  $$
- What are the rules to simplify functions by omitting dominated terms? #card
	- Multiplicative constants can be ommited: $14n^2 = n^2$
	  logseq.order-list-type:: number
	- $n^a \text { dominates } n^b \text { if } a > b$: $n^2$ dominates $n$
	  logseq.order-list-type:: number
	- Any exponential dominates any polynomial: $3^n$ dominates $n^5$
	  logseq.order-list-type:: number
	- Any polynomial dominates any logarithm; $n^2$ dominates $log(n)^3$; $n^2$ dominates $nlog(n)$
	  logseq.order-list-type:: number
- Steps for calculating efficiency: #card
	- Identify the algorithms basic operations
	  logseq.order-list-type:: number
		- Operations repeated at the core of the algorithm
		  logseq.order-list-type:: number
			- Comparisons
			- Swapping
			- Multiplications
			- Additions
	- Find an equation that tallies these basic operations as a function of n
	  logseq.order-list-type:: number
	- Solve for the equation
	  logseq.order-list-type:: number
- Some Simple Math Tools: #card
	- $$
	  % Sum of first n natural numbers
	  \sum_{i=1}^{n} i = \frac{n(n+1)}{2} \\
	  
	  % Index shifting identity
	  \sum_{i=j}^{n} 1 = n - j + 1 \\
	  
	  % Constant factor rule
	  \sum_{i=1}^{u} c = c \cdot \sum_{i=1}^{u} 1 = c \cdot u \\
	  
	  % Linearity of summation
	  \sum_{i=1}^{u} c_i \pm b_i = \sum_{i=1}^{u} c_i \pm \sum_{i=1}^{u} b_i \\
	  
	  % Sum of i from 0 to n
	  \sum_{i=0}^{n} i = \sum_{i=1}^{n} i = \frac{n(n+1)}{2} \approx \frac{n^2}{2} \in \Theta(n^2) \\
	  $$
- How does calculations for recursive relations different to sequential relations? #card
	- They need to be solved either using backwards substitution or the Master theorem
- What is a recurrence relation? #card
	- Recursive Mathematical function
	- Needs a base case
	- Recursive case
- There are two main recurrence relation #card
	- $T(n) = aT(n-k) + f(n)$
	- $T(n) = aT(n\div b) + f(n)$
- What is backwards substitution? #card
	- Using the piecewise function to find the Counts
	- e.g.
		- Say you have a piecewise:
		  $$
		  F(n) = 
		  \begin{cases}
		    0 & \text{if } n = 0 \\
		    F(n-1) * 1   & \text{otherwise}
		  \end{cases}
		  $$
		- Which translates to
		- ```
		  fun f(n) {
		  if n == 0 return 0 << Base case
		  return f(n - 1) + 1 << Recursive case
		  }
		  ```
		- List operations (Multiplications):
			- Base case: F(0) -> Multiplications (M) = 0; $M(0) = 0$
			- Recursive case: $M(n) = M(n -1) + 1$
				- Because $M(n - 1) + 1 = [M(n - 2) + 1] + 1$ (The substitution)
				- We can derive that $M(n - k) + k$ is the general formula for the nested recursion
				-
- What are the different conditions for the maseter theorem? #card
	- For $T(n) = aT(n \div b) + f(n) \text { where } n \in \Theta(n^d) \text { and } T(1) = c$
		- $a < b^d$
			- $T(n) \in \Theta(n^d)$
		- $a = b$
			- $T(n) \in (n^d log n)$
		- $a > b^d$
			- $T(n) \in (n^{log} b^a)$
- Define Brute Force algorithms #card
	- Straight forward approaches directly based on the problem statement
- What are the benefits and drawbacks of brute force algorithms? #card
	- Benefits
		- Quick and easy to implement
		- Simple and widely understood
		- Can yield reasonable algorithms for important problems and easy for simple tasks
	- Drawbacks
		- Often inefficient
		- Can be infeasible slow
	-
- High level Brute Force example for string matching #card
	- Align pattern at beginning of text
	  logseq.order-list-type:: number
	- Move from left to right (of the pattern), and compare the chars on pattern and input until
	  logseq.order-list-type:: number
		- You've moved through the pattern and everything matches
		  logseq.order-list-type:: number
		- There is a mismatch
		  logseq.order-list-type:: number
	- While pattern is not matched, after every step 2, shift one char to the right
	  logseq.order-list-type:: number
- Pseudocode of the Brute Force string mathcing Algorithm #card
	- ```
	  BruteForceStringMatch ( T[0..n-1], P[0..m-1] )
	  // T is the text; P is the pattern we‚Äôre searching
	  // for in the text
	  for k ‚Üê 0 to n ‚Äì m do // for each char in T
	  j ‚Üê 0
	  while j < m and P[j] = T[k+j] do // for each char in P
	  j ‚Üê j + 1
	  if j = m return k
	  return -1
	  ```
- What is the worst case for the brute force string matching algorithm? #card
	- Every char matches except for the last one for the whole time:
		- aaaaaaaaaa > aaab
- What is the brute force closest pair approach: #card
	- ```
	  dmin ‚Üê ‚àû
	  for i ‚Üê 1 to n-1 do
	  for j ‚Üê i+1 to n do
	  d ‚Üê sqrt((xi ‚Äì x j)2 + (y i - y j )2 )
	  if d < dmin
	  dmin = d
	  index1 = i
	  index2 = j
	  return index1, index2
	  ```
- What define exhaustive search #card
	- A brute force solution to searching for an element with a particular property
	- Ususally among perutations or combinations
	- Generate each element in the problem domain
- What is the brute force method for exhaustive searching? #card
	- Construct a way to list all potential solutions to the problem systematically
	  logseq.order-list-type:: number
	- Evaluate the solutions one by one, tracking the best one
	  logseq.order-list-type:: number
	- When the search ends, return winner
	  logseq.order-list-type:: number
- Define the #travellingsalesman #problem #card
	- Conditions
		- Given n cities with known distances between each pair
	- Requirement
		- Find the shortest tour  that passes through all cities exactly once before returning to the starting city
	- How else can the travelling salesman problem be describes? #card
		- Find the Hamiltonial Circuit in a weighted graph
- What is the #bruteforce/solution to the #travellingsalesman ? #card
	- Find a way to list all potential solutions
		- Use a graph algorithm to transverse through all the possible paths
	- Evaluate each solution one by one
		- Look at the cost of each tour
	- Return winner
		- Return lowest cost
- Define the #knapsack #problem #card
	- Conditions
		- Given $n$ items
		- With $w_1, w_2, w_3 .. w_n$ weights
		- With $v_1, v_2, v_3 .. v_n$ values
		- And a knapsack capacity of $W$
	- Requirements
		- Find the most valuable subset of items that can fit into the knapsack
- What is the #bruteforce/solution to the #knapsack #probelm? #card
	- List all potential solutions
		- List every combination of items
	- Evaluate every solution
		- Check the total weights are < $W$
	- Return answer
		- Find the largest
- Define the #assignement #problem #card
	- Contitions
		- $n$ people need $n$ jobs to be done
		- Each person is assigned 1 job
		- The cost of person $i$ doing job $j$ is $C[i,j]$
	- Requirement
		- Find the job assignment with the lowest cost
- What is the #bruteforce/solution to the #assignment #problem? #card
	- List all solutions
		- Generate all permutations of n integers
	- Evaluate every solution
		- Compute the cost of each solution
	- Return cheapest
- How do graphs relate to exhaustive search? #card
	- Exhaustive search transverses through all vertices of the graph
	- Two main transversals
		- Depth first search
		- Breadth first search
	- Transversal time for both is
		- $\Theta(|V|^2)$ for adjacency **matrix** representation
		- $\Theta(|V| + |E|)$ for adjacency **list** representation
- Characteristics of Exhaustive search #card
	- Time is infeasible in most cases
	- Most likely a better solution
	- Sometime the only solution
	- Parallel solutions can speed it up
- Define Decrease and Conquer strategies #card
	- Solve smaller instances of the problem
	- Extend solution to the larger main problem
- What are the 3 variants of Decrease and Conquer strategies? #card
	- Decrease by a constant
	- Decrease by a constant factor
	- Variable size decrease
- How is decrease and conquer different to divide and conquer? #card
	- Decrease and conquer *throws away* what it does not need to work out
	- Divide and Conquer does both
- List the Decrease by a constant algorithms #card
	- Insertion sort
	- Graph searching
	- Generating permutations and subsets
	- Topological sorts
- List the Decrease by constant factor algorithms #card
	- Binary Search
	- Fake coin probelm
	- Multiplication a la Russe
- List the Variable size decrease algorithms #card
	- Euclids algorithm
	- Interpolation Search
	- Finding the kth order statistic
- What are the strengths and weaknesses of decrease and conquer algorithms? #card
	- Strengths
		- Can be implemented top down or bottom up
		- Often efficient
	- Weaknesses
		- Less widely applicable
- Describe the #decreasebyconstant/solution for Insertion Sort #card
	- Assume there is already a sorted list of size $n-1$
	- Insert the remaining element in the correct position
	- But first perform the insertion sort on smaller list
- How is insertion sort a bottom up algorithm? #card
	- We recurse **we start with base case (the empty array)**, then start the sorting operations on that array
- How is bottom up different from top down? #card
	- Top down approach would take the entire array (problem) and break it down before moving back up
	- The direction of reasoning for bottom up is that we don't deal with the problem at once
	- The direction of reasoning for top-down is that we break the problem down
- Insertion sort algorithm #card
	- ```
	  insertionSort(Array arr, int n)
	  if (n <= 1) return; // base case
	  insertionSort( arr, n-1 );
	  // insert last element
	  last = arr[n-1]; j = n-2;
	  while (j >= 0 and arr[j] > last)
	  arr[j+1] = arr[j];
	  j = j - 1;
	  arr[j+1] = last
	  ```
- Describe the #topologicalsort #problem #card
	- Conditions
		- In a DAG, all edges have a direction
		- When listing the vertices, the vertex needs to appear before its destination in the list
			- This means that A -> B and B -> A is not possible because who comes first?
		- Sometime there are multiple orderings
		- Can use Depth First searching
	- Requirement
		- List all vertices in a DAG such that each V precedes it's destinations in the list
- Describe the #decreasebyconstant/solution for #topologicalsort #card
	- Repeatedly remove a source vertex, it's incident and outgoing edges
	  logseq.order-list-type:: number
	- Append to a sorted list
	  logseq.order-list-type:: number
	- If no source exists, there is a cycle or we are done
	  logseq.order-list-type:: number
- Describe the #generatingpowerset #problem #card
	- Requirement
		- Generate all $2^n$ subsets of a set
		- Order does not matter, but no duplicates
- Describe the #decreasebyconstant/solution for #generatingpowerset #card
	- Start with the base case: {}
	  logseq.order-list-type:: number
	- Recursively decrease set size by removing one element on each recursion
	  logseq.order-list-type:: number
	- Append children when calling back up:
	  logseq.order-list-type:: number
		- So {} will get appended with a -> {a}
		  logseq.order-list-type:: number
			- {a} gets added to total set -> [{}, {a}]
			  logseq.order-list-type:: number
		- {} and {a} gets appended with b -> {b}, {a, b}
		  logseq.order-list-type:: number
			- {b}, and {a, b} gets added to total -> [{}, {a}, {b}, {a, b}]
			  logseq.order-list-type:: number
		- {}, {a}, {b}, {a, b} gets appended with c -> {c}, {a, c}, {b, c}, {a, b, c}
		  logseq.order-list-type:: number
			- These get appended to total again -> [{}, {a}, {b}, {a, b}, {c}, {a, c}, {b, c}, {a, b, c}]
			  logseq.order-list-type:: number
- Describe the #bruteforce/solution for #generatingpowerset #card
	- For each value in the set, assign a bit value
	  logseq.order-list-type:: number
	- Then in a larger loop, we count from 0 to $2^n -1$
	  logseq.order-list-type:: number
		- This counting is done in bits so we get:
		  logseq.order-list-type:: number
			- 000
			  logseq.order-list-type:: number
			- 001
			  logseq.order-list-type:: number
			- 010
			  logseq.order-list-type:: number
			- 011
			  logseq.order-list-type:: number
			- 100
			  logseq.order-list-type:: number
			- 101
			  logseq.order-list-type:: number
			- 110
			  logseq.order-list-type:: number
			- 111
			  logseq.order-list-type:: number
	- In the inner look we basically check to see which values should be included so the mapping would go:
	  logseq.order-list-type:: number
		- 000: {}
		  logseq.order-list-type:: number
		- 001: {0}
		  logseq.order-list-type:: number
		- 010: {1}
		  logseq.order-list-type:: number
		- 011: {0, 1}
		  logseq.order-list-type:: number
		- 100: {2}
		  logseq.order-list-type:: number
		- 101: {2, 0}
		  logseq.order-list-type:: number
		- 110: {2, 1}
		  logseq.order-list-type:: number
		- 111: {2, 1, 0}
		  logseq.order-list-type:: number
- How does the minimal change version of the #generatingpowerset algorithm differ from the regular iterative one? #card
	- Instead of counting top to bottom in binary regularly, we just change one bit everytime
- Define the #generatepermutations #problem #card
	- Condition
		- All permutations need to be unique
	- This is a necessary component of exhaustive search
	- Requirement
		- Find all possible orderings of numbers in range {1..n}
- Define the #decreasebyconstant/solution for #generatepermutations #card
	- Recursively generate all permutations for child sets (that exclude current set)
	  logseq.order-list-type:: number
	- Insert n into all possible solutions and append the solution
	  logseq.order-list-type:: number
	- This is similar to the generate powerset solution
- For generating permutations, how can the minimal change requirement be satisfied? #card
	- Inserting from left and right alternately before moving on allows the next change to simply swap the elements around instead of shifting
	- Example
		- 1
		- Insert 2: 12 | 21 // insert from R
		- Insert 3: 123 | 132 | 312 | |321 | 231 | 213 // ins from R, then L
	- In the above example, the 3 "moves" across the array from one direction in the L tree ending at 312, so that when we evaluate the R tree, we just swap the bits instead of starting over
- Define the goals of the Johnson Trotter method: #card
	- Avoid permutations of smaller lists
	- By using arrows to keep track of the next permutation
- What are the rules of the Johnson Trotter method #card
	- Element K is mobile if it points to an element smaller than itself
- Define the Johnson Trotter algorithm #card
	- ```
	  Initialize the first permutation with:
	  ‚Üê ‚Üê ‚Üê
	  1 2 ‚Ä¶ n
	  while last permutation has mobile elem:
	  - find its largest mobile element k
	  - swap k with neighbour it points to
	  - reverse direction of elements > k
	  - add the new permutation to the list
	  Return the list of permutation
	  ```
- Define the #fakecoin #problem #card
	- Conditions
		- Among n coins, one is fake and weighs less
		- We have a scale that can compare any two **sets** of coins
	- Requirement
		- Find the fake coin
- Define the 2 pile #decreasebyconstantfactor/solution to the #fakecoin problem #card
	- Divide the set into two
	  logseq.order-list-type:: number
		- set a coin aside if odd
		  logseq.order-list-type:: number
	- Weigh them and discard the heavier set
	  logseq.order-list-type:: number
	- Proceed until coin set is 1
	  logseq.order-list-type:: number
- Define the 3 pile #decreasebyconstantfactor/solution to the #fakecoin problem #card
	- Divide the set into three
	  logseq.order-list-type:: number
	- Weigh the first two and discard if they are the same, otherwise use the lighter pile
	  logseq.order-list-type:: number
	- Proceed until coin set is 0
	  logseq.order-list-type:: number
- Define the #decreasebyconstantfactor/solution for #multiplicationalaruss #card
	- logseq.order-list-type:: number
	  $$
	  f(n,m) =
	  \begin{cases}
	  \dfrac{n}{2}\,(2m) & \text{if $n$ is even},\\[8pt]
	  \dfrac{n-1}{2}\,(2m) + m & \text{if $n$ is odd}.
	  \end{cases}
	  $$
	- This decreases by a constant factor (2)
	  logseq.order-list-type:: number
- Define #euclidgcd #problem #card
	- Requirement
		- Find the largest integer that divides n and m exactly
- Define #variablesizedecrease/solution for Euclid GCD #card
	- gcd(m, n) = gcd(n, m mod n)
	- gcd(m, 0) = m
	- Right-side args are smaller by neither a constant size nor factor
- Define the #findkthorderstatistic #problem #card
	- Conditions
		- It may not be possible to sort a list
		- Median is when k = n/2
	- Requirement
		- Find the kth smallest element in a list
- Define the #variablesizedecrease/solution to the #findkthorderstatistic problem #card
	- Pick a pivot point
	  logseq.order-list-type:: number
	- Partition array into 3 sets:
	  logseq.order-list-type:: number
		- Less than
		  logseq.order-list-type:: number
		- equal to
		  logseq.order-list-type:: number
		- Greater than
		  logseq.order-list-type:: number
	- If the pivot p fulfils the condition, we are done
	  logseq.order-list-type:: number
	- Otherwise, check which array would fulfil the condition are recurse
	  logseq.order-list-type:: number
- Define the Interpolation Search algorithm #card
	- Conditions
		- Assumes that values in the array increase linearly
	- Set up a straight like in the array for $(x_1; A[x_1])$ through to $(x_2; A[x_2])$
	  logseq.order-list-type:: number
		- Where $y = A[i]$ and $x = i$
		  logseq.order-list-type:: number
	- Find a proposed index k on the x-axis
	  logseq.order-list-type:: number
		- If A[k] is what we looking for, then we are done
		  logseq.order-list-type:: number
		- Otherwise split and your new area is:
		  logseq.order-list-type:: number
			- The standard interpolation-search probe position x within indices low and high for key k in an array A (with A[low] ‚â§ k ‚â§ A[high]) is:
			  logseq.order-list-type:: number
			  
			  \[
			  x = \text{low} + \left\lfloor
			  \frac{(k - A[\text{low}])}{(A[\text{high}] - A[\text{low}])}
			  \times (\text{high} - \text{low})
			  \right\rfloor
			  \]
			  
			  If you prefer without the floor:
			  
			  \[
			  x = \text{low} + \frac{(k - A[\text{low}])}{(A[\text{high}] - A[\text{low}])} \cdot (\text{high} - \text{low})
			  \]
			- low := x + 1 if A[x] < k, or
			  logseq.order-list-type:: number
			- high := x - 1 if A[x] > k.
			  logseq.order-list-type:: number
- Define the MergeSort algorithm #card
	- Split the Array until one element remains in both branches
	  logseq.order-list-type:: number
	- Compare $A_L[0]$ to $A_R[0]$
	  logseq.order-list-type:: number
	- Copy the smaller into the output array and append with the other
	  logseq.order-list-type:: number
	- Recursively propagate these to parent
	  logseq.order-list-type:: number
- Define the QuickSort algorithm #card
	- Pick a pivot, and move it out of the way
	  logseq.order-list-type:: number
	- Partition the array around your pivot
	  logseq.order-list-type:: number
		- By the end of this, you should have an array where only your pivot is in the correct place
		  logseq.order-list-type:: number
		- But we know that all $A[p_i]$ are smaller than p for i < p and larger for i > p
		  logseq.order-list-type:: number
	- For each array around your pivot, recursively use partitioning from the top to bottom
	  logseq.order-list-type:: number
- Define the Strassen method #card
	- ```
	  STRASSEN(A, B):
	  1. If n == 1 Output A11 √ó B11
	  2. Else
	  3. Split matrices into 8 n/2 X n/2 parts: A11 , B11 , . . . , A22 , B22
	  4. P1 = Strassen(A11, B12 ‚àí B22 )
	  5. P2 = Strassen(A11 + A12 , B22 )
	  6. P3 = Strassen(A21 + A22 , B11 )
	  7. P4 = Strassen(A22 , B 21 ‚àí B11 )
	  8. P5 = Strassen(A11 + A22 , B11 + B22 )
	  9. P6 = Strassen(A12 ‚àí A22 , B21 + B22 )
	  10 P7 = Strassen(A11 ‚àí A21 , B11 + B12 )
	  11. C 11 = P5 + P4 ‚àí P2 + P6
	  12. C 12 = P1 + P2
	  13. C 21 = P3 + P4
	  14. C 22 = P1 + P5 ‚àí P3 ‚àí P7
	  15. Output C
	  ```
- Describe the #divideandconquer/solution for Close Pair #card
	- 1. Sort points according to their x-
	  coordinates
	- 2. Split the set of points into two equal-sized
	  subsets by a vertical line x=x median
	- 3. Solve the problem recursively in the left
	  and right subsets, so that we get the left-
	  side and right-side minimum distances d l
	  and d r . d min =min(d l , dr )
	- 4. Straddle. Find the minimal distance in the
	  set S of points of width 2d around the
	  vertical line. Update d min if necessary
- Define the #divideandconquer/solution for Quick Hull #card
	- Identify leftmost and rightmost points P1
	  and P2
	- Compute upper hull:
		- Find point Pmax that is farthest away from line P1P2
		- Quickhull the points to the left of line P1Pmax
		- Quickhull the points to the left of line PmaxP2
	- Similarly compute lower hull
- What are the 3 types of transformations that can be made in Transform and Conquer #card
	- Instance Simplification
		- What are examples of instance Simplification #card
			- Presorting
			- Gaussian elimination
	- Representation Change
		- What are examples of Representation Change? #card
			- Balanced search trees
			- Heaps and Heapsort
			- Polynomial evaluation by Horners rule
			- Binary exponentiation
	- Problem reduction
		- What are examples of Problem reduction #card
			- Lowest common multiple
			- Reductions to graphs
- Define the Gaussian elimination algorithm #card
	- ```
	   BetterForwardElimination(A[1..n, 1..n], b[1..n])
	  //Implements Gaussian elimination with partial pivoting
	  //Input: Matrix A[1..n, 1..n] and column-vector b[1..n]
	  //Output: An equivalent upper-triangular matrix in place of A and the
	  //corresponding right-hand side values in place of the (n + 1)st column
	  for i ‚Üê 1 to n do A[i, n + 1]‚Üê b[i] //appends b to A as the last column
	  for i ‚Üê 1 to n ‚àí 1 do
	  pivotrow ‚Üê i
	  for j ‚Üê i + 1 to n do
	  if |A[j, i]| > |A[pivotrow, i]| pivotrow ‚Üê j
	  for k ‚Üê i to n + 1 do
	  swap(A[i, k], A[pivotrow, k])
	  for j ‚Üê i + 1 to n do
	  temp ‚Üê A[j, i] / A[i, i]
	  for k ‚Üê i to n + 1 do
	  A[j, k]‚Üê A[j, k] ‚àí A[i, k] ‚àó temp
	  ```
- Where is Presorting valuable? #card
	- Searching
	- Computing median
	- Finding repeated elements
	- Convex Hull and Closest pair
- What is the efficiency of Pre sorting #card
	- Overhead of \Theta(n log n)
- When is presorting worth it? #card
	- Closest pair ‚Äì Œò(n2) ‚Üí Œò (n log n), so presort
	- Checking uniqueness ‚Äì Brute force is Œò (n2 ), so presort
	- Finding the min and max ‚Äì Brute force is Œò (n), so NO presort
	- Finding the median ‚Äì Brute force is Œò (n2 ), so presort
	- Searching unordered array ‚Äì Brute force is Œò (n), so NO presort
- Define the #bruteforce/solution for polynomial evaluation #card
	- simlpy substitute directly
- How does horners rule work? #card
	- Factorise everything
	- then substitute
	- Leads to fewer multiplications by spreading x out
- Horners Rule pseudocode: #card
	- ```
	  double horner(coefficients[0..n], x):
	  p = coefficients[n]
	  for i = n ‚Äì 1 downto 0:
	  p = x * p + coefficients[i]
	  return p
	  ```
- Define the binary exponentiation algorithm #card
	- result = 1
	- base = a
	- exponent = n
	- while exponent > 0:
		- if exponent & 1 == 1: result = result * base
		- base = base * base
		- exponent = exponent >> 1
	- return result
- How is the LCM problem reduced: #card
	- The LCM of two positive integers m and n is the smallest integer
	  divisible by both m and n
	- Problem Reduction: LCM(m, n) = m * n / GCD(m, n) Example:
	  LCM(24, 60) = 1440 / 12 = 120
- Explain sort by counting #card
	- Sort a list whose elements fall in a restricted range of
	  id:: 69161407-d272-46da-afc1-cd1572527593
	  integers [L ‚Ä¶ U]
	- Using a frequency table that counts the number of
	  occurrences of each element
		- With U-L entries in the table
	- And a distribution table derived from frequencies to
	  tell us where to place elements
- Sort by counting algorithm #card
	- ```
	  for j ‚Üê 0 to U-L do D[j] ‚Üê 0 // init freq
	  for j ‚Üê 0 to n-1 do D[A[i]-L] ‚Üê D[A[i]-L]+1 // calc freq
	  for j ‚Üê 0 to U-L do D[j] ‚Üê D[j-1]+D[j] // calc distrib
	  for i ‚Üê n-1 downto 0 do
	  j ‚Üê A[i] - L
	  S[D[j]-1] ‚Üê A[i]
	  D[j] ‚Üê D[j]-1
	  return S
	  ```
- Define hoorspools algorithm #card
	- Construct a shift table T
	  logseq.order-list-type:: number
	- Align the pattern against the beginning of the text
	  logseq.order-list-type:: number
	- Repeat until match or pattern reaches end text:
	  logseq.order-list-type:: number
		- Starting with the last character of the pattern compare
		  the corresponding characters in the text until either all m
		  characters matched; or mismatch found
		- On mismatch retrieve T(c) where c is character in text
		  aligned to last character in pattern. Shift pattern right T(c)
		  positions.
- How does Boyer Moore differ to Hoorspool #card
	- In addition to bad-symbol shift table
	- Uses a good-suffix shift table with same idea applied to the
	  number of matched character
- What preprocessing is needed by Boyer Moore algorithm #card
	- Before we start searching we know everything about the
	  pattern and nothing about the text
	- The shift table is set up ONLY using information from the pattern
	- Both algorithms try to extract useful information from the
	  pattern in advance of the search, in order to maximise the
	  size of the shift they do on each mismatch
	- This is the point: The shift table is determined solely by the
	  properties of the pattern, not the text
- What is the good suffix rule #card
	- This is the second shift rule calculated by Boyer Moore
		- Uses suff(k) = k characters at end of pattern
		- Boyer Moore selects whichever shift is greater of the good suffix versus
		  bad symbol shift rule
	- Build a shift table G(k), ‚Äúgood suffix table‚Äù, that depends on
	  number of character matches, k
	- Several Cases when building G(k):
		- Case 1 ‚Äì The matching suff(k) does not occur elsewhere in the pattern
		- Case 2 ‚Äì The matching suff(k) occurs earlier in the pattern
		- Case 3 ‚Äì A part of the matching suff(k) occurs at the beginning of the
		  pattern
- Explain case 1 of Boyer Moore #card
	- Matched suffix, suff(k), does not occur elsewhere in the
	  pattern (k = the number of matched chars = suff(k)
		- Example:
		- Text: . . A B A B . . .
		- Pattern: M A O B A B
		- Determine G(k = 3)
		- Consider the suffix BAB, which is G(3); there is no earlier BAB in
		  pattern
		- So we can shift the entire pattern by m: G(3) = m = 6
- Explain case 2 of Boyer Moore #card
	- The matched suffix, suff(k), occurs earlier in the pattern
	- NB: search from right to left in pattern for another instance
	- Case 2A:
		- Text: . . . . A B A B . . .
		- Pattern: O B A B O B A B
		- Shift: O B A B O B A B
		- Same mismatch letter recurs in pattern so shift past last match, G(k=3) = 8
	- Case 2B:
		- Text: . . . . A B A B . . .
		- Pattern: A B A B O B A B
		- Shift: A B A B O B A B
		- Different mismatch letter, so shift to align last suff(k): G(k=3) = 4
- Explain case 3 of Boyer Moore algorithm #card
	- A part of the matching suffix occurs at the beginning
	  of the pattern ‚Äì find longest such match
	- NOTE: must match from end of suffix
	- Example:
		- Text: . . . . . A B A B . . .
		- Pattern: A B C B A B
		- Shift: A B C B A B
		- Here the front letters (prefix) match part of the suffix
		- So we can only shift G(k=3) = 4
- Define the boyer more algorithm #card
	- Good-suffix shift:
		- If k (> 0) symbols matched before failing, fetch d2 = G(k)
	- Bad-symbol shift:
		- If k (>= 0) symbols matched before failing, fetch
			- d1 = max(T(c) ‚Äì k, 1)
			- (where c is the character that didn‚Äôt match)
	- Algorithm:
	- Build tables T and G
	  logseq.order-list-type:: number
	- When searching, use max of d1and d2 (note requirement on k
	  logseq.order-list-type:: number
	  for use of d2
- What is the principle of dynamic programming #card
	- Solve problems by storing optimal solutions to
	  overlapping subproblems
		- Each subproblem solved only once and stored for lookup
		- Thus falls into the class of space-time tradeoffs
	- Solution derived from a recurrence relation
	  specification
	- Must obey principle of optimality
		- An overall optimal solution can be derived from optimal
		  solutions to sub-instance
- List of Dynamic programming algorithms #card
	- Introductory Algorithms
		- Fibonacci
		- Change-making
		- Coin collecting problem
	- Transitive Closure: Warshall‚Äôs Algorithm
	- All Pairs Shortest Path: Floyd‚Äôs Algorithm
	- Knapsack Problem
- Dynamic Algorithm for fibonacci #card
	- id:: 69161739-e2cf-467a-aacf-6d1516ef2b1b
	  ```
	  fibo(n):
	  F[0] = 0
	  F[1] = 1
	  for i = 2 to n do:
	  F[i] = F[i ‚Äì 1] + F[i ‚Äì 2]
	  return F[n]
	  ```
- Explain the change making problem #card
	- Given a collection of coin
	  denominations d 1 < d 2 < ‚Ä¶ < d m
		- Find exact change for the amount n
		  such that the minimum number of
		  coins is used
		- Each denomination has an unlimited
		  number
		  d1 = 1
- Recurrance relation for change making problem #card
	- Recurrence Relation
	- Add one coin that accounts for the largest part of n for different denominations
	- F(n) is minimum number of coins adding up to n
	- ùêπùêπ 0 = 0
	- ùêπùêπ ùëõùëõ = minùëóùëó:ùëõùëõ‚â•ùëëùëë ùëóùëó {ùêπùêπ(ùëõùëõ ‚àí ùëëùëëùëóùëó)} + 1 ùëìùëìùëìùëìùëìùëì ùëõùëõ > 0
- Change making algorithm #card
	- ```
	  F[0] ‚Üê 0
	  for i ‚Üê 1 to n do
	  temp‚Üê‚àû; j‚Üê1
	  while j ‚â§ m and i ‚â• D[j] do
	  temp ‚Üê min(F[i ‚àí D[j]], temp)
	  j‚Üêj+1
	  F[i] ‚Üê temp + 1
	  return F [n]
	  ```
- Describe the Coin counting problem informally #card
	- Given a board with n x m cells, some cells
	  have 1 coin placed (0 or 1 allowed)
	- A robot must start at top-left and collect coins
	  as it moves to bottom right cell
		- BUT robot can only move one cell to right, or one
		  cell down as it moves. Must pick up a coin in a cell
		  if it visits
	- Problem: Find maximum number of coins it
	  can collect and the path that it follow
- Describe the coin counting problem formally #card
	- Let F(i, j) be the largest number of coins the robot can collect and
	  bring to cell (i, j ) in the i th row and j th column of the board
		- Can reach cell (i,j)from ONLY (i-1,j) or (i, j-1) (i.e. left or above)
		- Largest # coins carried to those cells already are F(i-1,j) and F(i,j-1)
		- So, largest number of coins at (i, j) is the max of coins carried from a route going through either of those cells PLUS any coin added at cell (i,j)
		- We also need initial conditions: F(i,0) = 0, F(0,j) = 0 from 0th row and column
		  (not shown in table ‚Äì used to initialize row 1 and column
- Coin collecting algorithm solution explained #card
	- Fill board row by row by
	  evaluating recurrence
	- Time and space efficiency
	  Œò(nm)
	- Backtrace for path(s) ‚Äì start at
	  end goal state F(5,5) and see
	  how we got there
		- If F(i-1,j) > F(i,j-1), we came from
		  above
		- If F(i,j-1) > F(i-1,j), we came from,
		  left
		- If F‚Äôs the same we choose either
	- Backtrace: Œò(n+m)
- Define a transitive closure #card
	- For a directed graph with n vertices
	- Transitive closure is the n-by-n Boolean matrix T = {t ij} indicating reachability
		- If there is a 1 in the ith row and jth column then there is a directed path from vertex i to vertex j
		- If the entry is 0 then there is no path
- What is Warshalls Principle #card
	- Idea: if there‚Äôs a way to go from A to B, and also from B to C, then there‚Äôs a way to get from A to C
	- A path exists between two vertices i, j, iff there is
	  an edge from i to j; or
		- a path from i to j going through vertex 1; or
		- a path from i to j going through vertex 1 and/or 2; or
		- a path from i to j going through vertex 1, 2, and/or 3; or
		- ...
		- a path from i to j going through any of the other vertices
		- Successively build corresponding transition matrices R(0), R (1), ‚Ä¶,
		- R (n) = T
- Warshall's Algorithm #card
	- ```
	  R(0) ‚Üê adjacency matrix
	  for k ‚Üê 1 to n do
	  for i ‚Üê 1 to n do
	  for j ‚Üê 1 to n do
	  R(k)[i,j] ‚Üê R(k-1)[i,j] or (R(k-1)[i,k] and R(k-1)[k,j])
	  return R(n)
	  ```
- How does Floyd's Algorithm work #card
	- Initialisation:
		- dij(0) = wij direct
		- This is the weighted adjacency graph
		- No intermediate vertex from i to j
	- Iteration:
		- dij(k) = min(dij(k-1) , dik(k-1) + dkj(k-1) ) if k ‚â• 1
	- Termination:
		- Stop at k = n
		  All intermediate vertices are in set {1, 2,
		  ‚Ä¶, n}
- Floyd's algorithm code #card
	- ```
	  D ‚Üê W
	  for k ‚Üê 1 to n do
	  for i ‚Üê 1 to n do
	  for j ‚Üê 1 to n do
	  D[i,j] ‚Üê min( D[i,j], D[i,k] + D[k,j] )
	  return D
	  ```
- What is the dynamic programming solution for the knapsack problem #card
	- Sub-problem definition:
		- i items: 1 ‚â§ ùëñùëñ ‚â§ ùëõùëõ
		- Weights: ùë§ùë§1, ‚Ä¶ , ùë§ùë§ùëñùëñ
		- Values: ùë£ùë£1, ‚Ä¶ , ùë£ùë£ùëñùëñ
		- Knapsack size: j ‚Äì 1 ‚â§ ùëóùëó ‚â§ ùëäùëä
	- Data Structure:
		- Create a table V of sub-problem solutions
		- Where V[i,j] is the value of the optimal
		  subset of the first i items that fit into a
		  knapsack of size
- Knapsack memory function solution #card
	- ```
	  sack(i,j)
	  if V[i,j] < 0 // -1 is null
	  if j < w[i] then val ‚Üê sack(i-1,j)
	  else val ‚Üê max( sack(i-1,j),
	  v[i]+sack(i-1,j-w[i]))
	  V[i,j] ‚Üê val
	  return V[i,j]
	  ```
- Principle of greedy techniques #card
	- Optimization problems solved through a sequence of choices
	  that are:
		- Feasible ‚Äì satisfy problem constraints
		- Locally optimal ‚Äì best choice among all feasible options for that step
		- Irrevocable ‚Äì no backing out
	- A greedy grab of the best alternative, hoping that sequence of locally optimal steps will lead to a globally optimal solution
	- Sometimes approximation is acceptable (local optimality waived)
	- Not all optimizations can be solved this way
- What is the greedy algorithm for change making? #card
	- At any step choose the coin of the largest
	  denomination that doesn‚Äôt exceed the
	  remaining total; Repeat
	- Optimal for reasonable sets of coins
	- Think of an example where it isn‚Äôt optim
- What is a spanning tree #card
	- Spanning tree of an undirected connected graph is a tree of
	  connected nodes such that all graph vertices are contained on
- What is a minimal spanning tree #card
	- A minimum spanning tree is a spanning tree of ‚Äúminimal weight‚Äù (sum
	  of weights) for a graph with edge weights
	- Used in network communications, social media (and other) relationships, approx.
	  solution to TSP etc
- How do we greedily find the MST #card
	- Exhaustive construction is exponential ‚Äì instead,
	  we use a greedy approach. Two different greedy
	  algorithms:
	- Prim, adding vertices - O(|E| log |V|)
	- Kruskal, adding edges - O(|E| log |E
- What is text encoding requirements? #card
	- Assign bit patterns to an alphabet of characters so that in general texts fewer bits are used overall (entropy coding)
	- Fixed length
		- Same number of bits for each character
		- E.g., ASCII
	- Variable length
		- Number of bits for a character varies according to probability of occurrence
		- Leads to better compression
		- e.g., Morse‚Äôs telegram cod
- How does Huffman trees relate to text encoding? #card
	- How to determine which bits belong to which
	  character in a variable length encoding
	  Solved by Prefix-free codes where no code is
	  the prefix of another character
	- Huffman Tree:
		- Implements prefix-free codes
		- Leaves are characters, left edges encode 0-bits, right edges 1-bits
		- Walk the tree to encode and decode
		- Example encoding: ABADC ‚Üí 0100011101
		- Example decoding: 11000101 ‚Üí DAAA
- Huffman tree coding algorithm: #card
	- 1. Initialize n one-node trees labelled with the
	  characters of the alphabet
	- 2. Record the frequency/weight of the character in the
	  root
	- 3. REPEAT until a single tree is obtained:
		- Find two trees with the smallest weight
		- Make them the left and right sub-tree of a new
		  tree and record the sum of their weights in the
		  root
- Notes on Huffman Coding #card
	- Compression Ratio:
	  Standard measure of compression
	  CR = 100 * (y - x) / y, where x is compressed and y is
	  uncompressed
	  Typically, 20-80% in the case of Huffman Coding
	  Yields optimal compression provided:
	  Probabilities of character occurrences are independent
	  and are known in advance and powers of 2
	  BUT not great with bit strings: alphabet = {0,1}
	  More sophisticated approaches ‚Äì Adaptive Huffman,
	  builds frequency table on the fl
- Useful theorem #card
	- If t1(n) ‚àà O(g(n)) and t2(n) ‚àà O(h(n)) then
	  t1(n) + t2(n) ‚àà O(max{g(n), h(n)})
	  Same holds for Œò and Œ©
	  This allows us to estimate complexity for algorithms with
	  sequential parts of known complexity
	  Example: ‚Äúbubble sort‚Äù an array then find an element
	  Sorting: O(n2 )
	  Search: O(n)
	  Combined = O(max{n2 , n}) = O(n
- Big O (big oh)
  Running time as n gets larger is at most proportional to g(n)
  Upper bound
  Big Œ© (big omega)
  Running time as n gets larger is at least proportional to g(n)
  Lower bound
  Big Œò (big theta)
  Running time as n gets larger is exactly proportional to g(n)
  Exact bound
- Examples
  Average of O(n2 ) ‚Äì algorithm grows at most as fast as n2 with
  average case input
  E.g., Bubble sort
  Worst case of O(n3 ) ‚Äì algorithm grows at most as fast as n3 with its
  worst case
  E.g., brute force matrix multiplication, which is also Œò(n 3 )
  Best case of Œò(n) ‚Äì algorithm grows linearly in the best case
  E.g., sum an array
  Worst case of Œ©(2n ) ÔÉ† algorithm grows at best exponentially in
  worst case
  E.g., create the power set
- Checkpoint
  True or false: Œò (n + log n) = Œò (n)
  True or false: O (n + log n) = O (n)
  True or false: Œò (n log 2 n) = Œò (n log 10 n)
  True or false: Œò (log 2 n) = Œò (log n)
  True or false: O (n log n) = O (n)
  True or false: if x ‚àà O (n log n) then x ‚àà O (n2 )
  True or false: if x ‚àà Œò (n log n) then x ‚àà Œò (n2 )
  True or false: if x ‚àà O (n log n) then x ‚àà O (n
- Checkpoint Solutions
  True: Œò (n + log n) = Œò (n)
  We can discard addition terms that grow more slowly
  True: O (n + log n) = O (n)
  Same as above
  True: Œò (n log2 n) = Œò (n log 10 n)
  log2 n / log10 n = a constant = 3.32, approximately (log base change)
  False: Œò (log 2 n) = Œò (log n)
  Œò (log 2 n) grows more than a constant faster than Œò (log n)
  False: O (n log n) = O (n)
  These are two different sets. But O (n) is a subset of O (n log
- Checkpoint Solutions
  True: if x ‚àà O (n log n) then x ‚àà O (n 2 )
  E.g., Mergesort efficiency is in O (n log n). By definition it is
  also therefore in O(n2)
  False: if x ‚àà Œò (n log n) then x ‚àà Œò (n 2 )
  E.g. Mergesort is strictly Œò(n log n)
  False: if x ‚àà O (n log n) then x ‚àà O (n)
  x might be ‚àà O (n) but it might not be. E.g. Mergesort
- Show that Œò (n + log n) = Œò (n)
  Show that functions in Œò(n + log n) are actually in Œò(n)
  We know that Œò(2n) = Œò(n) (constants don‚Äôt matter)
  We also know:
  n + log n < n + n (because log n < n)
  => n + log n < 2n
  and we know that: n + log n > n
  So n < n + log n < 2n
  which means t(n) = n+log ‚àà Œò (n), thus Œò(n + log n) = Œò(n)
  i.e. these two sets of functions are equivalent in terms of asymptotic
  efficiency
- Analysis Exercise 1
  What does this algorithm do?
  What is its basic operation?
  In terms of n, how many times is its basic operation
  executed?
  What is its asymptotic efficiency class
  int MysteryFunction( A [0 .. n-1] )
  MysteryVal = A[0]
  for i = 1 to n ‚Äì 1:
  if A[i] > MysteryVal:
  MysteryVal = A[i]
  return MysteryVal
	- Analysis Solution 1
	  What does this algorithm do?
	  Finds the largest element in an array
	  What is its basic operation?
	  A[i] > maxVal (comparison)
	- In terms of n, how many times is its basic operation
	  executed?
	  What is its asymptotic efficiency class? ‚Äì Œò(n) (also big O!)
- Analysis Exercise 2: Is it a set
  You have loaded a file of strings into an array
  You want to check if there are any duplicates
  In other words, is the array, A, a set?
  i.e., each element appears at most once
  Come up with a few algorithms to determine if A is a
  set
  What are their time complexities?
  Which is more efficient
  Is it a set ‚Äì Brute Force
  What is the worst case?
  Both loops run to completion
  What are the core operations?
  comparison
  How many times does the inner loop execute?
  (n-1)2 comparisons
  boolean isset(A) {
  for (i=0; i<A.length; i++)
  for (j= 0; j<A.length; j++)
  if (i != j and A[i] == A[j])
  return false;
  return true;
- Is it a set ‚Äì Decrease & Conquer (iterative)
  What is the worst case? ‚Äì No early out, completion of both nested
  loops
  What are the core operations? ‚Äì comparisons
  How many times does the inner loop execute?
  n-1 + n-2 + n-3 ‚Ä¶ + 1 times = n(n-1)/2
  Worst case: Œò(n2 ). Average case too
  boolean isset(A) {
  for (i=0; i<A.length-1; i++)
  for (j=i+1; j<A.length; j++)
  if (A[i] == A[j]) return false;
  return true;
- Boolean isset(Array A, int index=0) {
  if (index >= A.length-1) return true;
  for (i=index+1; i<A.length; i++) {
  if (A[index]==A[i]) return false;
  }
  return isset(A, index+1);
  Is it a set ‚Äì Decrease & Conquer (recursive)
  Don't let the recursion scare you
  This algorithm is materially identical to the brute force one
  Worst case: Œò(n 2). Average case too
  We'll see how to do the maths for this late
- boolean isset(A) {
  sort(A);
  for (int i=0; i<A.length-1; i++)
  if (A[i] == A[i+1]) return false;
  return true;
  }
  Is it a set ‚Äì Transform & Conquer
  Here we sort the array and then do a linear search through it
  So which is more efficient? Transform & conquer or brute
  force
  Brute Force vs.
  Transform & Conquer
  What is the efficiency of the transform & conquer solution?
  The most efficient sorting algorithms are Œò(n log n)
  The for loop is linear for the worst case: Œò(n)
  It runs the sort, then the for loop. The asymptotic efficiency of this
  sequence of 2 computational steps is:
  Œò(max{n log n, n}) = Œò(n log n)
  So the Transform & Conquer algorithm is faster than the
  brute force solution of Œò(n2)
- Analysing Recursive Algorithms
  Recursive efficiency follows the same pattern as
  sequential:
  Determine core operation ‚á¢ create equation for number of
  repetitions ‚á¢ solve ‚á¢ derive efficiency
  Except that equations are recurrence relations and not
  summations
  Need to build a similar maths toolset for recurrence relations
  Either solve by Backward Substitution or using the Master
  Theorem (or closed form recurrence relation solutions -
  ignored, but see Appendices of Levitin
- Analysis Exercise 3: Factorial
  Basic operation we want to count?
  Multiplication
  How many multiplications?
  Once for every recursive call
  int F(n) {
  if n == 0 return 1;
  return F(n-1) * n;
	- Analysis Solution 3 by Backward Substitution
	  Let M(n) be number of multiplications
	  Then M(0) = 0
	  And M(n) = M(n-1) + 1
	  M(n) = M(n-1) +1 = [M(n-2) + 1] + 1
	  = [M(n-3) + 1] + 2 = M(n-3) + 3
	  And in general:
	  M(n) = M(n-k) + k, for k substitutions
	  Thus, when k = n (n substitutions)
	  M(n) = M(n-n) + n = M(0) + n = n
	  So M(n) ‚àà Œò (n) [it is exactly n in this case]
- Analysis Exercise 4: Counting Bits
  How do we determine the number of basic
  operations?
  Basic op is addition of 1 on each call to CountBits.
  int CountBits(int n such that n > 0):
  if n == 1 return 1
  else return 1 + CountBits(n / 2)
- Analysis Solution 4
  A(1) = 0 The addition doesn't take place when n=1
  A(n) = A(n / 2) + 1 for n > 1
  Actually need ùê¥ùê¥( ùëõùëõ
  2 )to get integer argument ‚Üí problem!
  BUT we can use a trick ‚Äì assume n is a power of 2:
  Let n = 2k which is the same as saying k = log2 n
  Now n/2 = (¬Ω) . 2k = 2-1 .2k = 2k-1
  A(1) = A(20 ) = 0
  A(n) = A(2k) = A(2k-1 ) + 1 for k > 0
  = [A(2k-2 ) + 1] + 1 = A(2k-2 ) + 2
  = A(2k-k) + k = A(20 ) + k = k = log2 n ‚àà Œò(log n)
  Extends to all positive n via ‚Äòsmoothness theorem
- Fibonacci Analysis
  Basic operation is an addition
  Recurrence relation:
  A(n) = A(n-1) + 1 + A(n-2)
  Requires a different solution method (uses ‚Äúcharacteristic
  equation‚Äù)
  A(n) ‚àà Œò(1.61803n ) (!)
  int F(n):
  if n<=1 return n
  else return F(n-1) + F(n-2)
- Master Theorem Motivation
  No general solution to all recurrence relations
  But the Master Theorem shows the asymptotic efficiency
  class (e.g. big theta or big O) of recurrence relations of the
  form:
  T(n) = aT(n/b) + f(n) where f(n) ‚àà Œò(n d )
  Provides a short cut that replaces backward substitution
- Simple Example of Master Theorem
  Consider: A(n) = A(n / 2) + 1, A(1) = 0
  For pattern: T(n) = aT(n/b) + f(n), f(n) ‚àà Œò(n d) and T(1) = c
  a = 1, b = 2, c = 0, and f(n) ‚àà Œò(1) = Œò(n 0 )
  Thus, d = 0
  Select Master Theorem Case:
  bd= 1
  Therefore a = bd
  By Master Theorem: T(n) ‚àà Œò(nd log n)
  And since d = 0, T(n) ‚àà Œò( log n)
  Another Simple Example
  Consider: A(n) = 2 A(n/2) + 1, A(1) = 1
  For pattern T(n) = aT(n/b) + f(n), f(n) ‚àà Œò(n d ) and T(1) = c
  a = 2, b = 2, c = 1
  and f(n) ‚àà Œò(1) = Œò(n 0), so d = 0
  Now 2 > 20 so a > b d
  By master theorem A(n) ‚àà Œò(n log b a) = Œò(n log 2 2) = Œò(n)
- String matching worst case
	- Worst Case
	  Worst case: the search string matches on every character
	  except the last, for every iteration of the outer loop
	  E.g., Text = ‚Äúaaaaaaaaaaaaaaaaaaaaaaa....‚Äù (n chars)
	  Search string = ‚Äúaaab‚Äù (m chars)
	  = m(n-m+1) character comparisons
	  = Œò(mn) for m much smaller than n (which is what happens in
	  practice)
	  Worst case very unlikely with natural language!
	  Average case on natural language? ‚Äì Œò(n)
- Closest Pair Algorithm
  Efficiency: Œò(n2)
  Sqrt evaluation is expensive in practice but can be avoided
  dmin ‚Üê ‚àû
  for i ‚Üê 1 to n-1 do
  for j ‚Üê i+1 to n do
  d ‚Üê sqrt((xi ‚Äì x j)2 + (y i - y j )2 )
  if d < dmin
  dmin = d
  index1 = i
  index2 = j
  return index1, index
- Brute Force Convex Hull
  Problem:
  Find the convex hull enclosing n
  2D points
  Convex Hull: If S is a set of points
  then the Convex Hull of S is the
  smallest convex set containing S
  Convex Set: A set of points in the
  plane is convex if for any two
  points P and Q, the line segment
  joining P and Q belongs to the set
- Convex Hull Algorithm
  Algorithm:
  For each pair of points p1 and p 2
  Determine whether all other points lie to the same side of
  the straight line through p 1 and p 2
  They then form part of the convex hull
  Efficiency: Œò(n 3
- MergeSort Example
  Master Theorem:
  T(n) = aT(n/b) + f(n), f(n) ‚àà Œò(nd)
  T(n) ‚àà Œò(nd log n) if a = bd
  Recurrence (assume n=2k ):
  C(n) = 2 C(n/2) + C merge(n) for
  n > 1, C(1) = 0
  Cmerge(n) = n - 1 in the worst case
  Efficiency (all cases):
  a = bd (2 = 2 1 ) in Master Theorem
  Œò(n log n)
  Pro: stable (vs quicksort,
  heapsort)
  Con: uses extra space Œò(n)
- QuickSort (Reminder)
  Select a pivot (partitioning element) ‚Äì this is A[0] usually
  Rearrange the list into two sub-lists
  All elements positioned before the pivot are ‚â§ the pivot
  Those positioned after the pivot are > the pivot
  Requires a pivoting algorithm
  Exchange the pivot with the last element in the first sub-list
  The pivot is now in its final position
  QuickSort the two sub-lists
- Worst-Case Efficiency of QuickSort
  In the worst case all splits are completely skewed
  For instance, an already sorted list (ascending order)!
  One subarray is empty, other reduced by only one:
  Make n+1 comparisons
  Exchange pivot with itself
  Quicksort left = √ò, right = A[1..n-1]
  C worst = (n+1) + n + ... + 3 = (n+1)(n+2)/2 - 3 = Œò (n2
- General Efficiency of QuickSort
  id:: 69163a7e-87c9-4f03-8542-7c8256fc92b7
  While worst case is Œò (n2), best case (split in the middle) is
  Œò (n log n) and average case (random split) is Œò (n log n)
  Improvements (in combination 20-25% faster):
  Better pivot selection: median of three partitioning avoids worst
  case in sorted lists
  Switch to Insertion Sort on small subsets
  Elimination of recursion
  Considered the method of choice for internal sorting for
  large lists (n ‚â• 10000)
- Closest Pair Illustrated: Split & Solve
  Recursively find
  shortest pair in
  each half
  Let d min =
  min(d l ,dr )
  dmin = dr here
- Closest Pair Illustrated: Straddle
  Search for potential
  new d min among
  points within d of
  dividing line
  Efficiency:
  T(n) = 2T(n/2) + f(n),
  f(n) ‚àà Œò(n) (merge
  + split)
  a=2, b=2, d=1
  a = bd in Master
  Theorem
  ÔÉ† Œò(n log n)
  Same cost as pre-
  sorting step
	- Limits in the Straddle Zone
	  Initially hard to see how this can differ in efficiency from
	  brute force
	  What if S has almost as many points in it as the non-S part?
	  Then for each point in S we compare it to ALL points in front of it by
	  Brute Force:
	  for i = 0 to n ‚Äì 2:
	  for j = i + 1 to n-1: etc
	  Efficiency ‚Äì Œò(n2 )
	  But there is actually a property of S that dramatically limits
	  the number of points we have to look a
- Straddle
  Assuming point in S are sorted by
  increasing y. We need only compare
  against the next 5 points
  Proved via ‚Äúpacking argument‚Äù
  Given the restriction that points to left and
  right of the dividing line must be at least d =
  dmin apart
  Configuration on the right represents the
  closest packing
  It is not possible for a 6th point to be closer
  to P than d in the sweep direction
- QuickHull
  Solve the Convex Hull problem in an
  approach reminiscent of QuickSort
  Worst case: Œò(n2 ), but average is
  Œò(n)
  Solution
  Identify leftmost and rightmost points P1
  and P2
  Compute upper hull:
  ‚Ä¢ Find point Pmax that is farthest away from line
  P1P2
  ‚Ä¢ Quickhull the points to the left of line P1Pmax
  ‚Ä¢ Quickhull the points to the left of line PmaxP2
  Similarly compute lower hull 20
  P1
  Pmax P2
  Triangle Area = 0.5*|det(P1,P2,Pmax)
- Is Presorting Better?
  Is transformation by sorting better than brute force?
  Sorting is Œò (n log n) so transformation to sorting
  is only worthwhile if other algorithms are less efficient
  Closest pair ‚Äì Œò(n2) ‚Üí Œò (n log n), so presort
  Checking uniqueness ‚Äì Brute force is Œò (n2 ), so presort
  Finding the min and max ‚Äì Brute force is Œò (n), so NO presort
  Finding the median ‚Äì Brute force is Œò (n2 ), so presort
  Searching unordered array ‚Äì Brute force is Œò (n), so NO presort
- Example: Presorted Selection
  Find the kth smallest element in A[1],..., A[n]
  Special cases:
  Minimum: k = 1, Maximum: k = n, Median: k = ùëõùëõ/2
  Presorting-based algorithm:
  Partition-based algorithm
  (Variable Decrease & Conquer):
  
  Sort list
  Return A[k]
  Pivot at A[s] using Partitioning from Quicksort
  if s=k return A[s]
  else if s<k repeat with sublist A[s+1],..., A[n]
  else if s>k repeat with sublist A[1],..., A[s-1
- Notes on the Selection Problem
  Presorting-based algorithm:
  Œò(n log n) + Œò (1) = Œò (n log n)
  Also identifies the k smallest elements (not just the kth)
  Partition-based algorithm (Variable decrease & conquer):
  Worst case: Œò(n2 )
  Best case: Œò(n)
  Average case: Œò(n)
  Simpler linear (brute force) algorithm is better in the case of
  max & min
- Horner‚Äôs Rule
  Factor x out as much as possible
  Example:
  ùëùùëù ùë•ùë• = 2ùë•ùë• 4 ‚àí ùë•ùë• 3 + 3ùë•ùë• 2 + ùë•ùë• ‚àí 5
  = (2ùë•ùë• 3 ‚àí ùë•ùë• 2 + 3ùë•ùë• + 1) ùë•ùë• ‚àí 5
  = ( 2ùë•ùë• 2 ‚àí ùë•ùë• + 3 ùë•ùë• + 1) ùë•ùë• ‚àí 5
  = ( (2ùë•ùë• ‚àí 1) ùë•ùë• + 3 ùë•ùë• + 1) ùë•ùë• ‚àí 5
  How does this help?
- Evaluating Polynomials
  Polynomial of form
  ùëùùëù ùë•ùë• = ùëéùëé ùëõùëõùë•ùë•ùëõùëõ + ‚ãØ + ùëéùëéùëñùëñ ùë•ùë• ùëñùëñ + ‚ãØ+ ùëéùëé0
  Example:
  ùëùùëù ùë•ùë• = 2ùë•ùë• 4 ‚àí ùë•ùë• 3 + 3ùë•ùë• 2 + ùë•ùë• ‚àí 5
  Evaluate for ùë•ùë• = 3
  The traditional, obvious, brute force approach:
  ùëùùëù 3 = 2(3)4 ‚àí33 + 3 3 2 + 3 ‚àí 5
- Horner‚Äôs Rule Pseudocode
  Can you think of a polynomial where Horner‚Äôs Rule is
  no help at all?
  double horner(coefficients[0..n], x):
  p = coefficients[n]
  for i = n ‚Äì 1 downto 0:
  p = x * p + coefficients[i]
  return
- Horner‚Äôs Rule Efficiency
  Basic operations are multiplication and addition
  Let number of multiplications = M(n)
  Let number of additions = A(n)
  ùëÄùëÄ ùëõùëõ = ùê¥ùê¥ ùëõùëõ = ‚àëùëñùëñ=0
  ùëõùëõ‚àí1 1 = ùëõùëõ
  For the entire polynomial it takes as many
  multiplications as the Brute Force method uses for its
  first term
  Brute force needs n + (n-1) + ‚Ä¶ + 1 = Œò(n2) for entire
  (complete) polynomial
- Binary Exponentiation
  Problem: solve an efficiently
  Solution uses the binary representation of n
  Represent n as a polynomial p(x), with x = 2
  ùëõùëõ = ùëùùëù ùë•ùë• = ùëèùëèùëòùëò ùë•ùë• ùëòùëò + ‚ãØ + ùëèùëèùëñùëñ ùë•ùë• ùëñùëñ + ‚ãØ+ ùëèùëè0
  Then coefficients of polynomial (ùëèùëèùëòùëò ‚Ä¶ ùëèùëèùëñùëñ ‚Ä¶ ùëèùëè0)are digits of
  the binary presentation
  Example:
  ùëõùëõ = 13, ùëùùëù 2 = 1 ‚àó 23 + 1 ‚àó 22 +0 ‚àó 2 + 1
  Binary 13 is 1 1 0 1
- an=ap(2)
  Horner‚Äôs Rule for p(2)
  p=1 // leading digit is
  // always 1
  for i = k-1 downto 0:
  p = 2 * p + C[i]
  Implications for a n = ap(2)
  ap =a1
  for i = k-1 downto 0:
  ap = a(2 * p + C[i])
  19
  ùëéùëé 2ùëùùëù+ùê∂ùê∂[ùëñùëñ] = ùëéùëé 2ùëùùëù √ó ùëéùëé ùê∂ùê∂[ùëñùëñ] = (ùëéùëé ùëùùëù)2√ó ùëéùëé ùê∂ùê∂[ùëñùëñ] = ÔøΩ(ùëéùëé ùëùùëù)2 ùëñùëñùëñùëñ ùê∂ùê∂ ùëñùëñ = 0
  (ùëéùëé ùëùùëù)2√ó ùëéùëé ùëñùëñùëñùëñ ùê∂ùê∂ ùëñùëñ = 1
  prod = prod*prod
  if C[i] == 1:
  prod = prod*a
- an=ap(2)
  Horner‚Äôs Rule for p(x)
  p=C[n]
  for i = n-1 downto 0:
  p = x*p + C[i]
  return p
  Horner‚Äôs Rule for a n
  p=a
  for i = k-1 downto 0:
  p = p*p
  if C[i] == 1:
  p = p*a
  return p
- Horner Exponentiation: example
  Solve an for n = 13 exploiting Horner‚Äôs Rule
  ùëùùëù ùë•ùë• = 1ùë•ùë• 3 + 1ùë•ùë• 2 + 0ùë•ùë• + 1 = ùë•ùë• + 1 ùë•ùë• + 0 ùë•ùë• + 1 at ùë•ùë• = 2
  C[]: 1 1 0 1
  p: 1 1*2+1=3 3*2+0=6 6*2+1=13
  ap: a1 (a1)2 *a1 (a3)2*a0 (a6)2*a1 = a13
  Since an*2+d = (an ) 2 * ad and d is only 0 or 1
- Efficiency of Horner Exponentiation
  Basic operation is multiplication, M(n)
  Brute force exponentiation:
  an = a*a‚Ä¶*a (n-1 times)
  M(n)=n-1 ‚àà Œò (n)
  Horner exponentiation:
  At most 2 multiplications on each loop iteration
  k = length of bit representation of n, so k‚àí1 = ùëôùëôùëôùëôùëôùëô2 ùëõùëõ
  ùëòùëò ‚àí 1 ‚â§ ùëÄùëÄ ùëõùëõ ‚â§ 2 ùëòùëò ‚àí 1
  ‚üπ M(n) ‚â§ 2 ùëôùëôùëôùëôùëôùëô2 ùëõùëõ ‚àà Œò (log n)
- Reduction to Graph Problems
  Reduce a problem to a graph-based solution
  Typically find solution path from one node to another
  State-Space Graphs:
  Vertices represent states and edges represent valid transitions between
  states
  Start and goal vertices
  Widely used in AI
  Example:
  River Crossing Puzzle [(P)easant, (w)olf, (g)oat, (c)abbage]
  Goat&wolf, and goat&cabbage cannot be left alone
  Get everyone across the river ‚Äì boat caries peasant + one more item
  Find path between given initial/end state; want minimal ‚Äòlength‚Äô path